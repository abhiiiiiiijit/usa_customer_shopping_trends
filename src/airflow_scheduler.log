nohup: ignoring input
/home/adminabhi/gitrepo/airflow_local_server/airflow-venv/lib/python3.10/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[2024-12-13T23:36:25.115+0100] {_client.py:1025} INFO - HTTP Request: GET https://apacheairflow.gateway.scarf.sh/scheduler?version=2.10.3&python_version=3.10&platform=Linux&arch=x86_64&database=sqlite&db_version=3.37&executor=SequentialExecutor "HTTP/1.1 200 OK"
[2024-12-13T23:36:25.326+0100] {executor_loader.py:254} INFO - Loaded executor: SequentialExecutor
[2024-12-13 23:36:25 +0100] [647] [INFO] Starting gunicorn 23.0.0
[2024-12-13 23:36:25 +0100] [647] [INFO] Listening at: http://[::]:8793 (647)
[2024-12-13 23:36:25 +0100] [647] [INFO] Using worker: sync
[2024-12-13 23:36:25 +0100] [648] [INFO] Booting worker with pid: 648
[2024-12-13T23:36:25.386+0100] {scheduler_job_runner.py:938} INFO - Starting the scheduler
[2024-12-13T23:36:25.388+0100] {scheduler_job_runner.py:945} INFO - Processing each file at most -1 times
[2024-12-13T23:36:25.394+0100] {manager.py:174} INFO - Launched DagFileProcessorManager with pid: 649
[2024-12-13T23:36:25.397+0100] {scheduler_job_runner.py:1852} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-13T23:36:25.400+0100] {settings.py:63} INFO - Configured default timezone UTC
/home/adminabhi/gitrepo/airflow_local_server/airflow-venv/lib/python3.10/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
[2024-12-13T23:36:25.426+0100] {manager.py:406} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2024-12-13 23:36:25 +0100] [650] [INFO] Booting worker with pid: 650
[2024-12-13T23:41:25.454+0100] {scheduler_job_runner.py:1852} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-13T23:46:25.493+0100] {scheduler_job_runner.py:1852} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-13T23:51:25.526+0100] {scheduler_job_runner.py:1852} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-13T23:56:25.639+0100] {scheduler_job_runner.py:1852} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-14T00:01:25.780+0100] {scheduler_job_runner.py:1852} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-14T00:06:25.818+0100] {scheduler_job_runner.py:1852} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-14T00:11:25.855+0100] {scheduler_job_runner.py:1852} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-14T00:16:25.888+0100] {scheduler_job_runner.py:1852} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-14T00:18:52.264+0100] {manager.py:537} INFO - DAG ingest_2_gcs is missing and will be deactivated.
[2024-12-14T00:18:52.268+0100] {manager.py:549} INFO - Deactivated 1 DAGs which are no longer present in file.
[2024-12-14T00:18:52.325+0100] {manager.py:553} INFO - Deleted DAG ingest_2_gcs in serialized_dag table
[2024-12-14T00:21:25.928+0100] {scheduler_job_runner.py:1852} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-14T00:24:59.666+0100] {manager.py:537} INFO - DAG retail is missing and will be deactivated.
[2024-12-14T00:24:59.668+0100] {manager.py:549} INFO - Deactivated 1 DAGs which are no longer present in file.
[2024-12-14T00:24:59.740+0100] {manager.py:553} INFO - Deleted DAG retail in serialized_dag table
[2024-12-14T00:25:47.715+0100] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: dag_upload_csv_to_gcs.upload_csv_to_gcs manual__2024-12-13T23:25:46.912022+00:00 [scheduled]>
[2024-12-14T00:25:47.717+0100] {scheduler_job_runner.py:495} INFO - DAG dag_upload_csv_to_gcs has 0/16 running and queued tasks
[2024-12-14T00:25:47.719+0100] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: dag_upload_csv_to_gcs.upload_csv_to_gcs manual__2024-12-13T23:25:46.912022+00:00 [scheduled]>
[2024-12-14T00:25:47.809+0100] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: dag_upload_csv_to_gcs.upload_csv_to_gcs manual__2024-12-13T23:25:46.912022+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-12-14T00:25:47.820+0100] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='dag_upload_csv_to_gcs', task_id='upload_csv_to_gcs', run_id='manual__2024-12-13T23:25:46.912022+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2024-12-14T00:25:47.821+0100] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dag_upload_csv_to_gcs', 'upload_csv_to_gcs', 'manual__2024-12-13T23:25:46.912022+00:00', '--local', '--subdir', 'DAGS_FOLDER/ingest_2_gcs.py']
[2024-12-14T00:25:47.837+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dag_upload_csv_to_gcs', 'upload_csv_to_gcs', 'manual__2024-12-13T23:25:46.912022+00:00', '--local', '--subdir', 'DAGS_FOLDER/ingest_2_gcs.py']
/home/adminabhi/gitrepo/airflow_local_server/airflow-venv/lib/python3.10/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
[2024-12-14T00:25:49.618+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/ingest_2_gcs.py
[2024-12-14T00:25:50.055+0100] {task_command.py:467} INFO - Running <TaskInstance: dag_upload_csv_to_gcs.upload_csv_to_gcs manual__2024-12-13T23:25:46.912022+00:00 [queued]> on host Abhijit.
[2024-12-14T00:25:50.836+0100] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dag_upload_csv_to_gcs', task_id='upload_csv_to_gcs', run_id='manual__2024-12-13T23:25:46.912022+00:00', try_number=1, map_index=-1)
[2024-12-14T00:25:50.842+0100] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=dag_upload_csv_to_gcs, task_id=upload_csv_to_gcs, run_id=manual__2024-12-13T23:25:46.912022+00:00, map_index=-1, run_start_date=2024-12-13 23:25:50.128982+00:00, run_end_date=2024-12-13 23:25:50.308511+00:00, run_duration=0.179529, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=28, pool=default_pool, queue=default, priority_weight=1, operator=LocalFilesystemToGCSOperator, queued_dttm=2024-12-13 23:25:47.786682+00:00, queued_by_job_id=27, pid=9579
[2024-12-14T00:25:50.906+0100] {dagrun.py:823} ERROR - Marking run <DagRun dag_upload_csv_to_gcs @ 2024-12-13 23:25:46.912022+00:00: manual__2024-12-13T23:25:46.912022+00:00, state:running, queued_at: 2024-12-13 23:25:46.971714+00:00. externally triggered: True> failed
[2024-12-14T00:25:50.907+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=dag_upload_csv_to_gcs, execution_date=2024-12-13 23:25:46.912022+00:00, run_id=manual__2024-12-13T23:25:46.912022+00:00, run_start_date=2024-12-13 23:25:47.563851+00:00, run_end_date=2024-12-13 23:25:50.907735+00:00, run_duration=3.343884, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-12-13 23:25:46.912022+00:00, data_interval_end=2024-12-13 23:25:46.912022+00:00, dag_hash=bbd5b99343a6fc66c7dd870fc83c4c4c
[2024-12-14T00:26:25.967+0100] {scheduler_job_runner.py:1852} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-14T00:27:29.389+0100] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: dag_upload_csv_to_gcs.upload_csv_to_gcs manual__2024-12-13T23:25:46.912022+00:00 [scheduled]>
[2024-12-14T00:27:29.389+0100] {scheduler_job_runner.py:495} INFO - DAG dag_upload_csv_to_gcs has 0/16 running and queued tasks
[2024-12-14T00:27:29.389+0100] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: dag_upload_csv_to_gcs.upload_csv_to_gcs manual__2024-12-13T23:25:46.912022+00:00 [scheduled]>
[2024-12-14T00:27:29.391+0100] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: dag_upload_csv_to_gcs.upload_csv_to_gcs manual__2024-12-13T23:25:46.912022+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-12-14T00:27:29.391+0100] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='dag_upload_csv_to_gcs', task_id='upload_csv_to_gcs', run_id='manual__2024-12-13T23:25:46.912022+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2024-12-14T00:27:29.392+0100] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dag_upload_csv_to_gcs', 'upload_csv_to_gcs', 'manual__2024-12-13T23:25:46.912022+00:00', '--local', '--subdir', 'DAGS_FOLDER/ingest_2_gcs.py']
[2024-12-14T00:27:29.406+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dag_upload_csv_to_gcs', 'upload_csv_to_gcs', 'manual__2024-12-13T23:25:46.912022+00:00', '--local', '--subdir', 'DAGS_FOLDER/ingest_2_gcs.py']
/home/adminabhi/gitrepo/airflow_local_server/airflow-venv/lib/python3.10/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
[2024-12-14T00:27:30.835+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/ingest_2_gcs.py
[2024-12-14T00:27:31.235+0100] {task_command.py:467} INFO - Running <TaskInstance: dag_upload_csv_to_gcs.upload_csv_to_gcs manual__2024-12-13T23:25:46.912022+00:00 [queued]> on host Abhijit.
[2024-12-14T00:27:33.915+0100] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dag_upload_csv_to_gcs', task_id='upload_csv_to_gcs', run_id='manual__2024-12-13T23:25:46.912022+00:00', try_number=2, map_index=-1)
[2024-12-14T00:27:33.920+0100] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=dag_upload_csv_to_gcs, task_id=upload_csv_to_gcs, run_id=manual__2024-12-13T23:25:46.912022+00:00, map_index=-1, run_start_date=2024-12-13 23:27:31.283057+00:00, run_end_date=2024-12-13 23:27:33.170941+00:00, run_duration=1.887884, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=29, pool=default_pool, queue=default, priority_weight=1, operator=LocalFilesystemToGCSOperator, queued_dttm=2024-12-13 23:27:29.390449+00:00, queued_by_job_id=27, pid=9969
[2024-12-14T00:27:34.083+0100] {dagrun.py:854} INFO - Marking run <DagRun dag_upload_csv_to_gcs @ 2024-12-13 23:25:46.912022+00:00: manual__2024-12-13T23:25:46.912022+00:00, state:running, queued_at: 2024-12-13 23:27:21.947260+00:00. externally triggered: True> successful
[2024-12-14T00:27:34.085+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=dag_upload_csv_to_gcs, execution_date=2024-12-13 23:25:46.912022+00:00, run_id=manual__2024-12-13T23:25:46.912022+00:00, run_start_date=2024-12-13 23:27:29.324061+00:00, run_end_date=2024-12-13 23:27:34.084379+00:00, run_duration=4.760318, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-12-13 23:25:46.912022+00:00, data_interval_end=2024-12-13 23:25:46.912022+00:00, dag_hash=f18368710cb4bd7ee6e886cbacf7a1b9
[2024-12-14T00:31:25.565+0100] {scheduler_job_runner.py:1852} INFO - Adopting or resetting orphaned tasks for active dag runs
