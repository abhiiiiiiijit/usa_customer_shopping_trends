nohup: ignoring input
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[2024-12-16T13:58:07.494+0100] {_client.py:1025} INFO - HTTP Request: GET https://apacheairflow.gateway.scarf.sh/scheduler?version=2.10.4&python_version=3.10&platform=Linux&arch=x86_64&database=sqlite&db_version=3.37&executor=SequentialExecutor "HTTP/1.1 200 OK"
[2024-12-16T13:58:07.669+0100] {executor_loader.py:254} INFO - Loaded executor: SequentialExecutor
[2024-12-16 13:58:07 +0100] [22466] [INFO] Starting gunicorn 23.0.0
[2024-12-16 13:58:07 +0100] [22466] [INFO] Listening at: http://[::]:8793 (22466)
[2024-12-16 13:58:07 +0100] [22466] [INFO] Using worker: sync
[2024-12-16 13:58:07 +0100] [22467] [INFO] Booting worker with pid: 22467
[2024-12-16T13:58:07.728+0100] {scheduler_job_runner.py:950} INFO - Starting the scheduler
[2024-12-16T13:58:07.729+0100] {scheduler_job_runner.py:957} INFO - Processing each file at most -1 times
[2024-12-16T13:58:07.736+0100] {manager.py:174} INFO - Launched DagFileProcessorManager with pid: 22468
[2024-12-16T13:58:07.738+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-16T13:58:07.741+0100] {settings.py:63} INFO - Configured default timezone UTC
[2024-12-16T13:58:07.757+0100] {scheduler_job_runner.py:1972} INFO - Marked 1 SchedulerJob instances as failed
[2024-12-16T13:58:07.775+0100] {manager.py:406} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2024-12-16 13:58:07 +0100] [22476] [INFO] Booting worker with pid: 22476
[2024-12-16T13:59:19.582+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: shopping_data_gcs_to_bq.create_sales_dataset manual__2024-12-14T14:58:07.170331+00:00 [scheduled]>
[2024-12-16T13:59:19.582+0100] {scheduler_job_runner.py:507} INFO - DAG shopping_data_gcs_to_bq has 0/16 running and queued tasks
[2024-12-16T13:59:19.583+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: shopping_data_gcs_to_bq.create_sales_dataset manual__2024-12-14T14:58:07.170331+00:00 [scheduled]>
[2024-12-16T13:59:19.585+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: shopping_data_gcs_to_bq.create_sales_dataset manual__2024-12-14T14:58:07.170331+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-12-16T13:59:19.586+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='shopping_data_gcs_to_bq', task_id='create_sales_dataset', run_id='manual__2024-12-14T14:58:07.170331+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-12-16T13:59:19.586+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'shopping_data_gcs_to_bq', 'create_sales_dataset', 'manual__2024-12-14T14:58:07.170331+00:00', '--local', '--subdir', 'DAGS_FOLDER/shopping_data_gcs_to_bq.py']
[2024-12-16T13:59:19.602+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'shopping_data_gcs_to_bq', 'create_sales_dataset', 'manual__2024-12-14T14:58:07.170331+00:00', '--local', '--subdir', 'DAGS_FOLDER/shopping_data_gcs_to_bq.py']
[2024-12-16T13:59:20.907+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/shopping_data_gcs_to_bq.py
[2024-12-16T13:59:22.074+0100] {task_command.py:467} INFO - Running <TaskInstance: shopping_data_gcs_to_bq.create_sales_dataset manual__2024-12-14T14:58:07.170331+00:00 [queued]> on host Abhijit.
[2024-12-16T13:59:24.452+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='shopping_data_gcs_to_bq', task_id='create_sales_dataset', run_id='manual__2024-12-14T14:58:07.170331+00:00', try_number=2, map_index=-1)
[2024-12-16T13:59:24.460+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=shopping_data_gcs_to_bq, task_id=create_sales_dataset, run_id=manual__2024-12-14T14:58:07.170331+00:00, map_index=-1, run_start_date=2024-12-16 12:59:22.124279+00:00, run_end_date=2024-12-16 12:59:23.702591+00:00, run_duration=1.578312, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=2, job_id=34, pool=default_pool, queue=default, priority_weight=2, operator=BigQueryCreateEmptyDatasetOperator, queued_dttm=2024-12-16 12:59:19.584204+00:00, queued_by_job_id=33, pid=22806
[2024-12-16T13:59:24.568+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: shopping_data_gcs_to_bq.load_sales_data_to_bq manual__2024-12-14T14:58:07.170331+00:00 [scheduled]>
[2024-12-16T13:59:24.569+0100] {scheduler_job_runner.py:507} INFO - DAG shopping_data_gcs_to_bq has 0/16 running and queued tasks
[2024-12-16T13:59:24.569+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: shopping_data_gcs_to_bq.load_sales_data_to_bq manual__2024-12-14T14:58:07.170331+00:00 [scheduled]>
[2024-12-16T13:59:24.570+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: shopping_data_gcs_to_bq.load_sales_data_to_bq manual__2024-12-14T14:58:07.170331+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-12-16T13:59:24.571+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='shopping_data_gcs_to_bq', task_id='load_sales_data_to_bq', run_id='manual__2024-12-14T14:58:07.170331+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2024-12-16T13:59:24.571+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'shopping_data_gcs_to_bq', 'load_sales_data_to_bq', 'manual__2024-12-14T14:58:07.170331+00:00', '--local', '--subdir', 'DAGS_FOLDER/shopping_data_gcs_to_bq.py']
[2024-12-16T13:59:24.586+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'shopping_data_gcs_to_bq', 'load_sales_data_to_bq', 'manual__2024-12-14T14:58:07.170331+00:00', '--local', '--subdir', 'DAGS_FOLDER/shopping_data_gcs_to_bq.py']
[2024-12-16T13:59:26.030+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/shopping_data_gcs_to_bq.py
[2024-12-16T13:59:27.082+0100] {task_command.py:467} INFO - Running <TaskInstance: shopping_data_gcs_to_bq.load_sales_data_to_bq manual__2024-12-14T14:58:07.170331+00:00 [queued]> on host Abhijit.
[2024-12-16T13:59:31.716+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='shopping_data_gcs_to_bq', task_id='load_sales_data_to_bq', run_id='manual__2024-12-14T14:58:07.170331+00:00', try_number=2, map_index=-1)
[2024-12-16T13:59:31.721+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=shopping_data_gcs_to_bq, task_id=load_sales_data_to_bq, run_id=manual__2024-12-14T14:58:07.170331+00:00, map_index=-1, run_start_date=2024-12-16 12:59:27.136037+00:00, run_end_date=2024-12-16 12:59:31.040689+00:00, run_duration=3.904652, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=2, job_id=35, pool=default_pool, queue=default, priority_weight=1, operator=GCSToBigQueryOperator, queued_dttm=2024-12-16 12:59:24.570062+00:00, queued_by_job_id=33, pid=22831
[2024-12-16T13:59:31.834+0100] {dagrun.py:854} INFO - Marking run <DagRun shopping_data_gcs_to_bq @ 2024-12-14 14:58:07.170331+00:00: manual__2024-12-14T14:58:07.170331+00:00, state:running, queued_at: 2024-12-16 12:59:15.954379+00:00. externally triggered: True> successful
[2024-12-16T13:59:31.835+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=shopping_data_gcs_to_bq, execution_date=2024-12-14 14:58:07.170331+00:00, run_id=manual__2024-12-14T14:58:07.170331+00:00, run_start_date=2024-12-16 12:59:19.476679+00:00, run_end_date=2024-12-16 12:59:31.835268+00:00, run_duration=12.358589, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-12-14 14:58:07.170331+00:00, data_interval_end=2024-12-14 14:58:07.170331+00:00, dag_hash=c823e093b5f898c6e907137832094f15
[2024-12-16T14:02:58.497+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-16T14:07:50.007+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-16T14:12:41.203+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-16T14:17:32.313+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-16T14:22:22.427+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-16T14:27:13.260+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-16T14:32:04.013+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-16T14:36:53.558+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-16T14:41:44.087+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-16T14:46:34.638+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-16T14:51:23.721+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-16T14:56:13.742+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-16T15:01:03.573+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-16T15:11:07.545+0100] {job.py:229} INFO - Heartbeat recovered after 329.73 seconds
[2024-12-16T15:11:17.845+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-16T15:16:10.519+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-16T15:21:03.429+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-16T15:25:53.505+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-16T15:30:44.139+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-16T15:35:34.151+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-16T15:40:22.978+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-16T15:44:37.743+0100] {job.py:229} INFO - Heartbeat recovered after 93.81 seconds
[2024-12-16T15:46:42.364+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-16T15:51:33.265+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-16T16:53:08.547+0100] {job.py:229} INFO - Heartbeat recovered after 3455.03 seconds
[2024-12-16T16:53:53.082+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-16T16:58:45.306+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-16T17:03:35.303+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-16T17:08:26.063+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-16T17:13:15.756+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-16T17:18:03.560+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
