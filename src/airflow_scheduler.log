nohup: ignoring input
/home/adminabhi/gitrepo/airflow_local_server/airflow-venv/lib/python3.10/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[2024-12-13T18:26:20.495+0100] {_client.py:1025} INFO - HTTP Request: GET https://apacheairflow.gateway.scarf.sh/scheduler?version=2.10.3&python_version=3.10&platform=Linux&arch=x86_64&database=sqlite&db_version=3.37&executor=SequentialExecutor "HTTP/1.1 200 OK"
[2024-12-13T18:26:20.592+0100] {executor_loader.py:254} INFO - Loaded executor: SequentialExecutor
[2024-12-13 18:26:20 +0100] [3805] [INFO] Starting gunicorn 23.0.0
[2024-12-13 18:26:20 +0100] [3805] [INFO] Listening at: http://[::]:8793 (3805)
[2024-12-13 18:26:20 +0100] [3805] [INFO] Using worker: sync
[2024-12-13 18:26:20 +0100] [3806] [INFO] Booting worker with pid: 3806
[2024-12-13T18:26:20.643+0100] {scheduler_job_runner.py:938} INFO - Starting the scheduler
[2024-12-13T18:26:20.645+0100] {scheduler_job_runner.py:945} INFO - Processing each file at most -1 times
[2024-12-13T18:26:20.652+0100] {manager.py:174} INFO - Launched DagFileProcessorManager with pid: 3807
[2024-12-13T18:26:20.655+0100] {scheduler_job_runner.py:1852} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-13T18:26:20.658+0100] {settings.py:63} INFO - Configured default timezone UTC
[2024-12-13 18:26:20 +0100] [3815] [INFO] Booting worker with pid: 3815
/home/adminabhi/gitrepo/airflow_local_server/airflow-venv/lib/python3.10/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
[2024-12-13T18:26:20.684+0100] {manager.py:406} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2024-12-13T18:27:03.178+0100] {dag.py:4180} INFO - Setting next_dagrun for example_bash_operator to 2024-12-13 00:00:00+00:00, run_after=2024-12-14 00:00:00+00:00
[2024-12-13T18:27:03.298+0100] {scheduler_job_runner.py:423} INFO - 5 tasks up for execution:
	<TaskInstance: example_bash_operator.runme_0 scheduled__2024-12-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: example_bash_operator.runme_1 scheduled__2024-12-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: example_bash_operator.runme_2 scheduled__2024-12-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: example_bash_operator.also_run_this scheduled__2024-12-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: example_bash_operator.this_will_skip scheduled__2024-12-12T00:00:00+00:00 [scheduled]>
[2024-12-13T18:27:03.298+0100] {scheduler_job_runner.py:495} INFO - DAG example_bash_operator has 0/16 running and queued tasks
[2024-12-13T18:27:03.298+0100] {scheduler_job_runner.py:495} INFO - DAG example_bash_operator has 1/16 running and queued tasks
[2024-12-13T18:27:03.299+0100] {scheduler_job_runner.py:495} INFO - DAG example_bash_operator has 2/16 running and queued tasks
[2024-12-13T18:27:03.299+0100] {scheduler_job_runner.py:495} INFO - DAG example_bash_operator has 3/16 running and queued tasks
[2024-12-13T18:27:03.299+0100] {scheduler_job_runner.py:495} INFO - DAG example_bash_operator has 4/16 running and queued tasks
[2024-12-13T18:27:03.299+0100] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: example_bash_operator.runme_0 scheduled__2024-12-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: example_bash_operator.runme_1 scheduled__2024-12-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: example_bash_operator.runme_2 scheduled__2024-12-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: example_bash_operator.also_run_this scheduled__2024-12-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: example_bash_operator.this_will_skip scheduled__2024-12-12T00:00:00+00:00 [scheduled]>
[2024-12-13T18:27:03.301+0100] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: example_bash_operator.runme_0 scheduled__2024-12-12T00:00:00+00:00 [scheduled]>, <TaskInstance: example_bash_operator.runme_1 scheduled__2024-12-12T00:00:00+00:00 [scheduled]>, <TaskInstance: example_bash_operator.runme_2 scheduled__2024-12-12T00:00:00+00:00 [scheduled]>, <TaskInstance: example_bash_operator.also_run_this scheduled__2024-12-12T00:00:00+00:00 [scheduled]>, <TaskInstance: example_bash_operator.this_will_skip scheduled__2024-12-12T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-12-13T18:27:03.302+0100] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='example_bash_operator', task_id='runme_0', run_id='scheduled__2024-12-12T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2024-12-13T18:27:03.302+0100] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'example_bash_operator', 'runme_0', 'scheduled__2024-12-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_bash_operator.py']
[2024-12-13T18:27:03.302+0100] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='example_bash_operator', task_id='runme_1', run_id='scheduled__2024-12-12T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2024-12-13T18:27:03.302+0100] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'example_bash_operator', 'runme_1', 'scheduled__2024-12-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_bash_operator.py']
[2024-12-13T18:27:03.302+0100] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='example_bash_operator', task_id='runme_2', run_id='scheduled__2024-12-12T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2024-12-13T18:27:03.303+0100] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'example_bash_operator', 'runme_2', 'scheduled__2024-12-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_bash_operator.py']
[2024-12-13T18:27:03.303+0100] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='example_bash_operator', task_id='also_run_this', run_id='scheduled__2024-12-12T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-12-13T18:27:03.303+0100] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'example_bash_operator', 'also_run_this', 'scheduled__2024-12-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_bash_operator.py']
[2024-12-13T18:27:03.303+0100] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='example_bash_operator', task_id='this_will_skip', run_id='scheduled__2024-12-12T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-12-13T18:27:03.303+0100] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'example_bash_operator', 'this_will_skip', 'scheduled__2024-12-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_bash_operator.py']
[2024-12-13T18:27:03.318+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'example_bash_operator', 'runme_0', 'scheduled__2024-12-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_bash_operator.py']
/home/adminabhi/gitrepo/airflow_local_server/airflow-venv/lib/python3.10/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
[2024-12-13T18:27:04.321+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/example_bash_operator.py
[2024-12-13T18:27:04.432+0100] {task_command.py:467} INFO - Running <TaskInstance: example_bash_operator.runme_0 scheduled__2024-12-12T00:00:00+00:00 [queued]> on host Abhijit.
[2024-12-13T18:27:06.153+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'example_bash_operator', 'runme_1', 'scheduled__2024-12-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_bash_operator.py']
/home/adminabhi/gitrepo/airflow_local_server/airflow-venv/lib/python3.10/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
[2024-12-13T18:27:07.193+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/example_bash_operator.py
[2024-12-13T18:27:07.345+0100] {task_command.py:467} INFO - Running <TaskInstance: example_bash_operator.runme_1 scheduled__2024-12-12T00:00:00+00:00 [queued]> on host Abhijit.
[2024-12-13T18:27:09.023+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'example_bash_operator', 'runme_2', 'scheduled__2024-12-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_bash_operator.py']
/home/adminabhi/gitrepo/airflow_local_server/airflow-venv/lib/python3.10/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
[2024-12-13T18:27:09.951+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/example_bash_operator.py
[2024-12-13T18:27:10.079+0100] {task_command.py:467} INFO - Running <TaskInstance: example_bash_operator.runme_2 scheduled__2024-12-12T00:00:00+00:00 [queued]> on host Abhijit.
[2024-12-13T18:27:11.787+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'example_bash_operator', 'also_run_this', 'scheduled__2024-12-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_bash_operator.py']
/home/adminabhi/gitrepo/airflow_local_server/airflow-venv/lib/python3.10/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
[2024-12-13T18:27:12.883+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/example_bash_operator.py
[2024-12-13T18:27:13.011+0100] {task_command.py:467} INFO - Running <TaskInstance: example_bash_operator.also_run_this scheduled__2024-12-12T00:00:00+00:00 [queued]> on host Abhijit.
[2024-12-13T18:27:13.663+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'example_bash_operator', 'this_will_skip', 'scheduled__2024-12-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_bash_operator.py']
/home/adminabhi/gitrepo/airflow_local_server/airflow-venv/lib/python3.10/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
[2024-12-13T18:27:14.632+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/example_bash_operator.py
[2024-12-13T18:27:14.739+0100] {task_command.py:467} INFO - Running <TaskInstance: example_bash_operator.this_will_skip scheduled__2024-12-12T00:00:00+00:00 [queued]> on host Abhijit.
[2024-12-13T18:27:15.570+0100] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='example_bash_operator', task_id='runme_0', run_id='scheduled__2024-12-12T00:00:00+00:00', try_number=1, map_index=-1)
[2024-12-13T18:27:15.570+0100] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='example_bash_operator', task_id='runme_1', run_id='scheduled__2024-12-12T00:00:00+00:00', try_number=1, map_index=-1)
[2024-12-13T18:27:15.571+0100] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='example_bash_operator', task_id='runme_2', run_id='scheduled__2024-12-12T00:00:00+00:00', try_number=1, map_index=-1)
[2024-12-13T18:27:15.571+0100] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='example_bash_operator', task_id='also_run_this', run_id='scheduled__2024-12-12T00:00:00+00:00', try_number=1, map_index=-1)
[2024-12-13T18:27:15.571+0100] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='example_bash_operator', task_id='this_will_skip', run_id='scheduled__2024-12-12T00:00:00+00:00', try_number=1, map_index=-1)
[2024-12-13T18:27:15.578+0100] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=example_bash_operator, task_id=also_run_this, run_id=scheduled__2024-12-12T00:00:00+00:00, map_index=-1, run_start_date=2024-12-13 17:27:13.066881+00:00, run_end_date=2024-12-13 17:27:13.274658+00:00, run_duration=0.207777, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=5, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-12-13 17:27:03.300271+00:00, queued_by_job_id=1, pid=4030
[2024-12-13T18:27:15.579+0100] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=example_bash_operator, task_id=runme_0, run_id=scheduled__2024-12-12T00:00:00+00:00, map_index=-1, run_start_date=2024-12-13 17:27:04.486501+00:00, run_end_date=2024-12-13 17:27:05.681905+00:00, run_duration=1.195404, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=2, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-12-13 17:27:03.300271+00:00, queued_by_job_id=1, pid=3982
[2024-12-13T18:27:15.579+0100] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=example_bash_operator, task_id=runme_1, run_id=scheduled__2024-12-12T00:00:00+00:00, map_index=-1, run_start_date=2024-12-13 17:27:07.396615+00:00, run_end_date=2024-12-13 17:27:08.617806+00:00, run_duration=1.221191, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=3, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-12-13 17:27:03.300271+00:00, queued_by_job_id=1, pid=4004
[2024-12-13T18:27:15.580+0100] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=example_bash_operator, task_id=runme_2, run_id=scheduled__2024-12-12T00:00:00+00:00, map_index=-1, run_start_date=2024-12-13 17:27:10.133928+00:00, run_end_date=2024-12-13 17:27:11.350458+00:00, run_duration=1.21653, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=4, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-12-13 17:27:03.300271+00:00, queued_by_job_id=1, pid=4015
[2024-12-13T18:27:15.580+0100] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=example_bash_operator, task_id=this_will_skip, run_id=scheduled__2024-12-12T00:00:00+00:00, map_index=-1, run_start_date=2024-12-13 17:27:14.793607+00:00, run_end_date=2024-12-13 17:27:15.030760+00:00, run_duration=0.237153, state=skipped, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=6, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-12-13 17:27:03.300271+00:00, queued_by_job_id=1, pid=4041
[2024-12-13T18:27:15.654+0100] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: example_bash_operator.run_after_loop scheduled__2024-12-12T00:00:00+00:00 [scheduled]>
[2024-12-13T18:27:15.655+0100] {scheduler_job_runner.py:495} INFO - DAG example_bash_operator has 0/16 running and queued tasks
[2024-12-13T18:27:15.655+0100] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: example_bash_operator.run_after_loop scheduled__2024-12-12T00:00:00+00:00 [scheduled]>
[2024-12-13T18:27:15.656+0100] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: example_bash_operator.run_after_loop scheduled__2024-12-12T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-12-13T18:27:15.657+0100] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='example_bash_operator', task_id='run_after_loop', run_id='scheduled__2024-12-12T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-12-13T18:27:15.657+0100] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'example_bash_operator', 'run_after_loop', 'scheduled__2024-12-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_bash_operator.py']
[2024-12-13T18:27:15.671+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'example_bash_operator', 'run_after_loop', 'scheduled__2024-12-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_bash_operator.py']
/home/adminabhi/gitrepo/airflow_local_server/airflow-venv/lib/python3.10/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
[2024-12-13T18:27:16.671+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/example_bash_operator.py
[2024-12-13T18:27:16.791+0100] {task_command.py:467} INFO - Running <TaskInstance: example_bash_operator.run_after_loop scheduled__2024-12-12T00:00:00+00:00 [queued]> on host Abhijit.
[2024-12-13T18:27:17.460+0100] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='example_bash_operator', task_id='run_after_loop', run_id='scheduled__2024-12-12T00:00:00+00:00', try_number=1, map_index=-1)
[2024-12-13T18:27:17.464+0100] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=example_bash_operator, task_id=run_after_loop, run_id=scheduled__2024-12-12T00:00:00+00:00, map_index=-1, run_start_date=2024-12-13 17:27:16.843722+00:00, run_end_date=2024-12-13 17:27:17.045784+00:00, run_duration=0.202062, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=7, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-12-13 17:27:15.655839+00:00, queued_by_job_id=1, pid=4053
[2024-12-13T18:27:17.498+0100] {dagrun.py:854} INFO - Marking run <DagRun example_bash_operator @ 2024-12-12 00:00:00+00:00: scheduled__2024-12-12T00:00:00+00:00, state:running, queued_at: 2024-12-13 17:27:03.170425+00:00. externally triggered: False> successful
[2024-12-13T18:27:17.499+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=example_bash_operator, execution_date=2024-12-12 00:00:00+00:00, run_id=scheduled__2024-12-12T00:00:00+00:00, run_start_date=2024-12-13 17:27:03.243244+00:00, run_end_date=2024-12-13 17:27:17.499699+00:00, run_duration=14.256455, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-12-12 00:00:00+00:00, data_interval_end=2024-12-13 00:00:00+00:00, dag_hash=6b57cae344b1385f6479a8b34a6c9b4e
[2024-12-13T18:27:17.504+0100] {dag.py:4180} INFO - Setting next_dagrun for example_bash_operator to 2024-12-13 00:00:00+00:00, run_after=2024-12-14 00:00:00+00:00
[2024-12-13T18:31:21.218+0100] {scheduler_job_runner.py:1852} INFO - Adopting or resetting orphaned tasks for active dag runs
