nohup: ignoring input
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[2025-01-10T17:02:36.727+0100] {_client.py:1025} INFO - HTTP Request: GET https://apacheairflow.gateway.scarf.sh/scheduler?version=2.10.4&python_version=3.10&platform=Linux&arch=x86_64&database=sqlite&db_version=3.37&executor=SequentialExecutor "HTTP/1.1 200 OK"
[2025-01-10T17:02:36.985+0100] {executor_loader.py:254} INFO - Loaded executor: SequentialExecutor
[2025-01-10 17:02:37 +0100] [62376] [INFO] Starting gunicorn 23.0.0
[2025-01-10 17:02:37 +0100] [62376] [INFO] Listening at: http://[::]:8793 (62376)
[2025-01-10 17:02:37 +0100] [62376] [INFO] Using worker: sync
[2025-01-10 17:02:37 +0100] [62377] [INFO] Booting worker with pid: 62377
[2025-01-10T17:02:37.066+0100] {scheduler_job_runner.py:950} INFO - Starting the scheduler
[2025-01-10T17:02:37.067+0100] {scheduler_job_runner.py:957} INFO - Processing each file at most -1 times
[2025-01-10T17:02:37.075+0100] {manager.py:174} INFO - Launched DagFileProcessorManager with pid: 62378
[2025-01-10T17:02:37.078+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-10T17:02:37.089+0100] {settings.py:63} INFO - Configured default timezone UTC
[2025-01-10 17:02:37 +0100] [62379] [INFO] Booting worker with pid: 62379
[2025-01-10T17:02:37.126+0100] {manager.py:406} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2025-01-10T17:06:45.342+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: shopping_data_gcs_to_bq.create_sales_dataset manual__2024-12-17T00:11:10.283324+00:00 [scheduled]>
[2025-01-10T17:06:45.344+0100] {scheduler_job_runner.py:507} INFO - DAG shopping_data_gcs_to_bq has 0/16 running and queued tasks
[2025-01-10T17:06:45.345+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: shopping_data_gcs_to_bq.create_sales_dataset manual__2024-12-17T00:11:10.283324+00:00 [scheduled]>
[2025-01-10T17:06:45.361+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: shopping_data_gcs_to_bq.create_sales_dataset manual__2024-12-17T00:11:10.283324+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-01-10T17:06:45.362+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='shopping_data_gcs_to_bq', task_id='create_sales_dataset', run_id='manual__2024-12-17T00:11:10.283324+00:00', try_number=3, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-01-10T17:06:45.363+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'shopping_data_gcs_to_bq', 'create_sales_dataset', 'manual__2024-12-17T00:11:10.283324+00:00', '--local', '--subdir', 'DAGS_FOLDER/shopping_data_gcs_to_bq.py']
[2025-01-10T17:06:45.380+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'shopping_data_gcs_to_bq', 'create_sales_dataset', 'manual__2024-12-17T00:11:10.283324+00:00', '--local', '--subdir', 'DAGS_FOLDER/shopping_data_gcs_to_bq.py']
[2025-01-10T17:06:47.689+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/shopping_data_gcs_to_bq.py
[2025-01-10T17:06:49.584+0100] {task_command.py:467} INFO - Running <TaskInstance: shopping_data_gcs_to_bq.create_sales_dataset manual__2024-12-17T00:11:10.283324+00:00 [queued]> on host Abhijit.
[2025-01-10T17:06:50.111+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='shopping_data_gcs_to_bq', task_id='create_sales_dataset', run_id='manual__2024-12-17T00:11:10.283324+00:00', try_number=3, map_index=-1)
[2025-01-10T17:06:50.116+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=shopping_data_gcs_to_bq, task_id=create_sales_dataset, run_id=manual__2024-12-17T00:11:10.283324+00:00, map_index=-1, run_start_date=2025-01-10 16:06:49.652606+00:00, run_end_date=2025-01-10 16:06:49.387914+00:00, run_duration=-0.264692, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=3, max_tries=3, job_id=37, pool=default_pool, queue=default, priority_weight=3, operator=BigQueryCreateEmptyDatasetOperator, queued_dttm=2025-01-10 16:06:45.359360+00:00, queued_by_job_id=36, pid=63526
[2025-01-10T17:06:50.308+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: shopping_data_gcs_to_bq.load_sales_data_to_bq manual__2024-12-17T00:11:10.283324+00:00 [scheduled]>
[2025-01-10T17:06:50.308+0100] {scheduler_job_runner.py:507} INFO - DAG shopping_data_gcs_to_bq has 0/16 running and queued tasks
[2025-01-10T17:06:50.308+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: shopping_data_gcs_to_bq.load_sales_data_to_bq manual__2024-12-17T00:11:10.283324+00:00 [scheduled]>
[2025-01-10T17:06:50.310+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: shopping_data_gcs_to_bq.load_sales_data_to_bq manual__2024-12-17T00:11:10.283324+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-01-10T17:06:50.310+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='shopping_data_gcs_to_bq', task_id='load_sales_data_to_bq', run_id='manual__2024-12-17T00:11:10.283324+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-01-10T17:06:50.310+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'shopping_data_gcs_to_bq', 'load_sales_data_to_bq', 'manual__2024-12-17T00:11:10.283324+00:00', '--local', '--subdir', 'DAGS_FOLDER/shopping_data_gcs_to_bq.py']
[2025-01-10T17:06:50.329+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'shopping_data_gcs_to_bq', 'load_sales_data_to_bq', 'manual__2024-12-17T00:11:10.283324+00:00', '--local', '--subdir', 'DAGS_FOLDER/shopping_data_gcs_to_bq.py']
[2025-01-10T17:06:51.507+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/shopping_data_gcs_to_bq.py
[2025-01-10T17:06:52.365+0100] {task_command.py:467} INFO - Running <TaskInstance: shopping_data_gcs_to_bq.load_sales_data_to_bq manual__2024-12-17T00:11:10.283324+00:00 [queued]> on host Abhijit.
[2025-01-10T17:06:56.413+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='shopping_data_gcs_to_bq', task_id='load_sales_data_to_bq', run_id='manual__2024-12-17T00:11:10.283324+00:00', try_number=1, map_index=-1)
[2025-01-10T17:06:56.417+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=shopping_data_gcs_to_bq, task_id=load_sales_data_to_bq, run_id=manual__2024-12-17T00:11:10.283324+00:00, map_index=-1, run_start_date=2025-01-10 16:06:52.416036+00:00, run_end_date=2025-01-10 16:06:55.715044+00:00, run_duration=3.299008, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=38, pool=default_pool, queue=default, priority_weight=2, operator=GCSToBigQueryOperator, queued_dttm=2025-01-10 16:06:50.309407+00:00, queued_by_job_id=36, pid=63553
[2025-01-10T17:06:56.581+0100] {dagrun.py:854} INFO - Marking run <DagRun shopping_data_gcs_to_bq @ 2024-12-17 00:11:10.283324+00:00: manual__2024-12-17T00:11:10.283324+00:00, state:running, queued_at: 2025-01-10 16:06:44.707299+00:00. externally triggered: True> successful
[2025-01-10T17:06:56.581+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=shopping_data_gcs_to_bq, execution_date=2024-12-17 00:11:10.283324+00:00, run_id=manual__2024-12-17T00:11:10.283324+00:00, run_start_date=2025-01-10 16:06:45.264351+00:00, run_end_date=2025-01-10 16:06:56.581621+00:00, run_duration=11.31727, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-12-17 00:11:10.283324+00:00, data_interval_end=2024-12-17 00:11:10.283324+00:00, dag_hash=24a96cd08548b42db8ee096bc35f6b8d
[2025-01-10T17:07:20.109+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-01-10T17:08:50.227+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: shopping_data_gcs_to_bq.create_sales_dataset manual__2024-12-14T14:58:07.170331+00:00 [scheduled]>
[2025-01-10T17:08:50.228+0100] {scheduler_job_runner.py:507} INFO - DAG shopping_data_gcs_to_bq has 0/16 running and queued tasks
[2025-01-10T17:08:50.228+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: shopping_data_gcs_to_bq.create_sales_dataset manual__2024-12-14T14:58:07.170331+00:00 [scheduled]>
[2025-01-10T17:08:50.230+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: shopping_data_gcs_to_bq.create_sales_dataset manual__2024-12-14T14:58:07.170331+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-01-10T17:08:50.231+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='shopping_data_gcs_to_bq', task_id='create_sales_dataset', run_id='manual__2024-12-14T14:58:07.170331+00:00', try_number=6, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-01-10T17:08:50.231+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'shopping_data_gcs_to_bq', 'create_sales_dataset', 'manual__2024-12-14T14:58:07.170331+00:00', '--local', '--subdir', 'DAGS_FOLDER/shopping_data_gcs_to_bq.py']
[2025-01-10T17:08:50.246+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'shopping_data_gcs_to_bq', 'create_sales_dataset', 'manual__2024-12-14T14:58:07.170331+00:00', '--local', '--subdir', 'DAGS_FOLDER/shopping_data_gcs_to_bq.py']
[2025-01-10T17:08:51.493+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/shopping_data_gcs_to_bq.py
[2025-01-10T17:08:52.390+0100] {task_command.py:467} INFO - Running <TaskInstance: shopping_data_gcs_to_bq.create_sales_dataset manual__2024-12-14T14:58:07.170331+00:00 [queued]> on host Abhijit.
[2025-01-10T17:08:52.149+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='shopping_data_gcs_to_bq', task_id='create_sales_dataset', run_id='manual__2024-12-14T14:58:07.170331+00:00', try_number=6, map_index=-1)
[2025-01-10T17:08:52.153+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=shopping_data_gcs_to_bq, task_id=create_sales_dataset, run_id=manual__2024-12-14T14:58:07.170331+00:00, map_index=-1, run_start_date=2025-01-10 16:08:52.440370+00:00, run_end_date=2025-01-10 16:08:51.484114+00:00, run_duration=-0.956256, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=6, max_tries=6, job_id=39, pool=default_pool, queue=default, priority_weight=3, operator=BigQueryCreateEmptyDatasetOperator, queued_dttm=2025-01-10 16:08:50.229068+00:00, queued_by_job_id=36, pid=63981
[2025-01-10T17:08:52.322+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: shopping_data_gcs_to_bq.load_sales_data_to_bq manual__2024-12-14T14:58:07.170331+00:00 [scheduled]>
[2025-01-10T17:08:52.323+0100] {scheduler_job_runner.py:507} INFO - DAG shopping_data_gcs_to_bq has 0/16 running and queued tasks
[2025-01-10T17:08:52.323+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: shopping_data_gcs_to_bq.load_sales_data_to_bq manual__2024-12-14T14:58:07.170331+00:00 [scheduled]>
[2025-01-10T17:08:52.325+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: shopping_data_gcs_to_bq.load_sales_data_to_bq manual__2024-12-14T14:58:07.170331+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-01-10T17:08:52.326+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='shopping_data_gcs_to_bq', task_id='load_sales_data_to_bq', run_id='manual__2024-12-14T14:58:07.170331+00:00', try_number=3, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-01-10T17:08:52.326+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'shopping_data_gcs_to_bq', 'load_sales_data_to_bq', 'manual__2024-12-14T14:58:07.170331+00:00', '--local', '--subdir', 'DAGS_FOLDER/shopping_data_gcs_to_bq.py']
[2025-01-10T17:08:52.342+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'shopping_data_gcs_to_bq', 'load_sales_data_to_bq', 'manual__2024-12-14T14:58:07.170331+00:00', '--local', '--subdir', 'DAGS_FOLDER/shopping_data_gcs_to_bq.py']
[2025-01-10T17:08:53.512+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/shopping_data_gcs_to_bq.py
[2025-01-10T17:08:54.381+0100] {task_command.py:467} INFO - Running <TaskInstance: shopping_data_gcs_to_bq.load_sales_data_to_bq manual__2024-12-14T14:58:07.170331+00:00 [queued]> on host Abhijit.
[2025-01-10T17:08:59.209+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='shopping_data_gcs_to_bq', task_id='load_sales_data_to_bq', run_id='manual__2024-12-14T14:58:07.170331+00:00', try_number=3, map_index=-1)
[2025-01-10T17:08:59.214+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=shopping_data_gcs_to_bq, task_id=load_sales_data_to_bq, run_id=manual__2024-12-14T14:58:07.170331+00:00, map_index=-1, run_start_date=2025-01-10 16:08:54.438959+00:00, run_end_date=2025-01-10 16:08:58.560340+00:00, run_duration=4.121381, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=3, max_tries=3, job_id=40, pool=default_pool, queue=default, priority_weight=2, operator=GCSToBigQueryOperator, queued_dttm=2025-01-10 16:08:52.324423+00:00, queued_by_job_id=36, pid=64006
[2025-01-10T17:08:59.389+0100] {dagrun.py:854} INFO - Marking run <DagRun shopping_data_gcs_to_bq @ 2024-12-14 14:58:07.170331+00:00: manual__2024-12-14T14:58:07.170331+00:00, state:running, queued_at: 2025-01-10 16:08:49.100264+00:00. externally triggered: True> successful
[2025-01-10T17:08:59.390+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=shopping_data_gcs_to_bq, execution_date=2024-12-14 14:58:07.170331+00:00, run_id=manual__2024-12-14T14:58:07.170331+00:00, run_start_date=2025-01-10 16:08:50.132146+00:00, run_end_date=2025-01-10 16:08:59.390181+00:00, run_duration=9.258035, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-12-14 14:58:07.170331+00:00, data_interval_end=2024-12-14 14:58:07.170331+00:00, dag_hash=24a96cd08548b42db8ee096bc35f6b8d
[2025-01-10T17:09:11.583+0100] {dagrun.py:854} INFO - Marking run <DagRun shopping_data_gcs_to_bq @ 2024-12-14 14:58:07.170331+00:00: manual__2024-12-14T14:58:07.170331+00:00, state:running, queued_at: 2025-01-10 16:09:10.742267+00:00. externally triggered: True> successful
[2025-01-10T17:09:11.584+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=shopping_data_gcs_to_bq, execution_date=2024-12-14 14:58:07.170331+00:00, run_id=manual__2024-12-14T14:58:07.170331+00:00, run_start_date=2025-01-10 16:09:11.562489+00:00, run_end_date=2025-01-10 16:09:11.584062+00:00, run_duration=0.021573, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-12-14 14:58:07.170331+00:00, data_interval_end=2024-12-14 14:58:07.170331+00:00, dag_hash=24a96cd08548b42db8ee096bc35f6b8d
[2025-01-10T17:09:54.811+0100] {dagrun.py:854} INFO - Marking run <DagRun shopping_data_gcs_to_bq @ 2024-12-17 00:11:10.283324+00:00: manual__2024-12-17T00:11:10.283324+00:00, state:running, queued_at: 2025-01-10 16:09:53.944171+00:00. externally triggered: True> successful
[2025-01-10T17:09:54.812+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=shopping_data_gcs_to_bq, execution_date=2024-12-17 00:11:10.283324+00:00, run_id=manual__2024-12-17T00:11:10.283324+00:00, run_start_date=2025-01-10 16:09:54.785816+00:00, run_end_date=2025-01-10 16:09:54.812142+00:00, run_duration=0.026326, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-12-17 00:11:10.283324+00:00, data_interval_end=2024-12-17 00:11:10.283324+00:00, dag_hash=24a96cd08548b42db8ee096bc35f6b8d
[2025-01-10T17:09:59.881+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: shopping_data_gcs_to_bq.create_sales_dataset manual__2025-01-10T16:09:58.590757+00:00 [scheduled]>
[2025-01-10T17:09:59.881+0100] {scheduler_job_runner.py:507} INFO - DAG shopping_data_gcs_to_bq has 0/16 running and queued tasks
[2025-01-10T17:09:59.881+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: shopping_data_gcs_to_bq.create_sales_dataset manual__2025-01-10T16:09:58.590757+00:00 [scheduled]>
[2025-01-10T17:09:59.883+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: shopping_data_gcs_to_bq.create_sales_dataset manual__2025-01-10T16:09:58.590757+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-01-10T17:09:59.883+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='shopping_data_gcs_to_bq', task_id='create_sales_dataset', run_id='manual__2025-01-10T16:09:58.590757+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-01-10T17:09:59.884+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'shopping_data_gcs_to_bq', 'create_sales_dataset', 'manual__2025-01-10T16:09:58.590757+00:00', '--local', '--subdir', 'DAGS_FOLDER/shopping_data_gcs_to_bq.py']
[2025-01-10T17:09:59.899+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'shopping_data_gcs_to_bq', 'create_sales_dataset', 'manual__2025-01-10T16:09:58.590757+00:00', '--local', '--subdir', 'DAGS_FOLDER/shopping_data_gcs_to_bq.py']
[2025-01-10T17:10:01.179+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/shopping_data_gcs_to_bq.py
[2025-01-10T17:10:02.077+0100] {task_command.py:467} INFO - Running <TaskInstance: shopping_data_gcs_to_bq.create_sales_dataset manual__2025-01-10T16:09:58.590757+00:00 [queued]> on host Abhijit.
[2025-01-10T17:10:03.780+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='shopping_data_gcs_to_bq', task_id='create_sales_dataset', run_id='manual__2025-01-10T16:09:58.590757+00:00', try_number=1, map_index=-1)
[2025-01-10T17:10:03.785+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=shopping_data_gcs_to_bq, task_id=create_sales_dataset, run_id=manual__2025-01-10T16:09:58.590757+00:00, map_index=-1, run_start_date=2025-01-10 16:10:02.134097+00:00, run_end_date=2025-01-10 16:10:03.019882+00:00, run_duration=0.885785, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=41, pool=default_pool, queue=default, priority_weight=3, operator=BigQueryCreateEmptyDatasetOperator, queued_dttm=2025-01-10 16:09:59.882453+00:00, queued_by_job_id=36, pid=64251
[2025-01-10T17:10:04.095+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: shopping_data_gcs_to_bq.load_sales_data_to_bq manual__2025-01-10T16:09:58.590757+00:00 [scheduled]>
[2025-01-10T17:10:04.096+0100] {scheduler_job_runner.py:507} INFO - DAG shopping_data_gcs_to_bq has 0/16 running and queued tasks
[2025-01-10T17:10:04.096+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: shopping_data_gcs_to_bq.load_sales_data_to_bq manual__2025-01-10T16:09:58.590757+00:00 [scheduled]>
[2025-01-10T17:10:04.098+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: shopping_data_gcs_to_bq.load_sales_data_to_bq manual__2025-01-10T16:09:58.590757+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-01-10T17:10:04.099+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='shopping_data_gcs_to_bq', task_id='load_sales_data_to_bq', run_id='manual__2025-01-10T16:09:58.590757+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-01-10T17:10:04.101+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'shopping_data_gcs_to_bq', 'load_sales_data_to_bq', 'manual__2025-01-10T16:09:58.590757+00:00', '--local', '--subdir', 'DAGS_FOLDER/shopping_data_gcs_to_bq.py']
[2025-01-10T17:10:04.116+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'shopping_data_gcs_to_bq', 'load_sales_data_to_bq', 'manual__2025-01-10T16:09:58.590757+00:00', '--local', '--subdir', 'DAGS_FOLDER/shopping_data_gcs_to_bq.py']
[2025-01-10T17:10:05.490+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/shopping_data_gcs_to_bq.py
[2025-01-10T17:10:06.380+0100] {task_command.py:467} INFO - Running <TaskInstance: shopping_data_gcs_to_bq.load_sales_data_to_bq manual__2025-01-10T16:09:58.590757+00:00 [queued]> on host Abhijit.
[2025-01-10T17:10:10.988+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='shopping_data_gcs_to_bq', task_id='load_sales_data_to_bq', run_id='manual__2025-01-10T16:09:58.590757+00:00', try_number=1, map_index=-1)
[2025-01-10T17:10:10.994+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=shopping_data_gcs_to_bq, task_id=load_sales_data_to_bq, run_id=manual__2025-01-10T16:09:58.590757+00:00, map_index=-1, run_start_date=2025-01-10 16:10:06.438025+00:00, run_end_date=2025-01-10 16:10:10.254962+00:00, run_duration=3.816937, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=42, pool=default_pool, queue=default, priority_weight=2, operator=GCSToBigQueryOperator, queued_dttm=2025-01-10 16:10:04.097683+00:00, queued_by_job_id=36, pid=64278
[2025-01-10T17:10:11.187+0100] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: shopping_data_gcs_to_bq.check_sales_dataset manual__2025-01-10T16:09:58.590757+00:00 [scheduled]>
[2025-01-10T17:10:11.188+0100] {scheduler_job_runner.py:507} INFO - DAG shopping_data_gcs_to_bq has 0/16 running and queued tasks
[2025-01-10T17:10:11.188+0100] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: shopping_data_gcs_to_bq.check_sales_dataset manual__2025-01-10T16:09:58.590757+00:00 [scheduled]>
[2025-01-10T17:10:11.190+0100] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: shopping_data_gcs_to_bq.check_sales_dataset manual__2025-01-10T16:09:58.590757+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-01-10T17:10:11.190+0100] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='shopping_data_gcs_to_bq', task_id='check_sales_dataset', run_id='manual__2025-01-10T16:09:58.590757+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-01-10T17:10:11.190+0100] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'shopping_data_gcs_to_bq', 'check_sales_dataset', 'manual__2025-01-10T16:09:58.590757+00:00', '--local', '--subdir', 'DAGS_FOLDER/shopping_data_gcs_to_bq.py']
[2025-01-10T17:10:11.207+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'shopping_data_gcs_to_bq', 'check_sales_dataset', 'manual__2025-01-10T16:09:58.590757+00:00', '--local', '--subdir', 'DAGS_FOLDER/shopping_data_gcs_to_bq.py']
[2025-01-10T17:10:12.460+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/shopping_data_gcs_to_bq.py
[2025-01-10T17:10:13.401+0100] {task_command.py:467} INFO - Running <TaskInstance: shopping_data_gcs_to_bq.check_sales_dataset manual__2025-01-10T16:09:58.590757+00:00 [queued]> on host Abhijit.
[2025-01-10T17:10:29.265+0100] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='shopping_data_gcs_to_bq', task_id='check_sales_dataset', run_id='manual__2025-01-10T16:09:58.590757+00:00', try_number=1, map_index=-1)
[2025-01-10T17:10:29.270+0100] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=shopping_data_gcs_to_bq, task_id=check_sales_dataset, run_id=manual__2025-01-10T16:09:58.590757+00:00, map_index=-1, run_start_date=2025-01-10 16:10:13.460009+00:00, run_end_date=2025-01-10 16:10:26.908860+00:00, run_duration=13.448851, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=43, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-01-10 16:10:11.189430+00:00, queued_by_job_id=36, pid=64317
[2025-01-10T17:10:29.511+0100] {dagrun.py:854} INFO - Marking run <DagRun shopping_data_gcs_to_bq @ 2025-01-10 16:09:58.590757+00:00: manual__2025-01-10T16:09:58.590757+00:00, state:running, queued_at: 2025-01-10 16:09:58.619589+00:00. externally triggered: True> successful
[2025-01-10T17:10:29.511+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=shopping_data_gcs_to_bq, execution_date=2025-01-10 16:09:58.590757+00:00, run_id=manual__2025-01-10T16:09:58.590757+00:00, run_start_date=2025-01-10 16:09:59.831496+00:00, run_end_date=2025-01-10 16:10:29.511800+00:00, run_duration=29.680304, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-01-10 16:09:58.590757+00:00, data_interval_end=2025-01-10 16:09:58.590757+00:00, dag_hash=24a96cd08548b42db8ee096bc35f6b8d
[2025-01-10T17:12:04.681+0100] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
