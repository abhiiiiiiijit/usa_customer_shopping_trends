nohup: ignoring input
/home/adminabhi/gitrepo/airflow_local_server/airflow-venv/lib/python3.10/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[2024-12-13T18:26:20.495+0100] {_client.py:1025} INFO - HTTP Request: GET https://apacheairflow.gateway.scarf.sh/scheduler?version=2.10.3&python_version=3.10&platform=Linux&arch=x86_64&database=sqlite&db_version=3.37&executor=SequentialExecutor "HTTP/1.1 200 OK"
[2024-12-13T18:26:20.592+0100] {executor_loader.py:254} INFO - Loaded executor: SequentialExecutor
[2024-12-13 18:26:20 +0100] [3805] [INFO] Starting gunicorn 23.0.0
[2024-12-13 18:26:20 +0100] [3805] [INFO] Listening at: http://[::]:8793 (3805)
[2024-12-13 18:26:20 +0100] [3805] [INFO] Using worker: sync
[2024-12-13 18:26:20 +0100] [3806] [INFO] Booting worker with pid: 3806
[2024-12-13T18:26:20.643+0100] {scheduler_job_runner.py:938} INFO - Starting the scheduler
[2024-12-13T18:26:20.645+0100] {scheduler_job_runner.py:945} INFO - Processing each file at most -1 times
[2024-12-13T18:26:20.652+0100] {manager.py:174} INFO - Launched DagFileProcessorManager with pid: 3807
[2024-12-13T18:26:20.655+0100] {scheduler_job_runner.py:1852} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-13T18:26:20.658+0100] {settings.py:63} INFO - Configured default timezone UTC
[2024-12-13 18:26:20 +0100] [3815] [INFO] Booting worker with pid: 3815
/home/adminabhi/gitrepo/airflow_local_server/airflow-venv/lib/python3.10/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
[2024-12-13T18:26:20.684+0100] {manager.py:406} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2024-12-13T18:27:03.178+0100] {dag.py:4180} INFO - Setting next_dagrun for example_bash_operator to 2024-12-13 00:00:00+00:00, run_after=2024-12-14 00:00:00+00:00
[2024-12-13T18:27:03.298+0100] {scheduler_job_runner.py:423} INFO - 5 tasks up for execution:
	<TaskInstance: example_bash_operator.runme_0 scheduled__2024-12-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: example_bash_operator.runme_1 scheduled__2024-12-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: example_bash_operator.runme_2 scheduled__2024-12-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: example_bash_operator.also_run_this scheduled__2024-12-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: example_bash_operator.this_will_skip scheduled__2024-12-12T00:00:00+00:00 [scheduled]>
[2024-12-13T18:27:03.298+0100] {scheduler_job_runner.py:495} INFO - DAG example_bash_operator has 0/16 running and queued tasks
[2024-12-13T18:27:03.298+0100] {scheduler_job_runner.py:495} INFO - DAG example_bash_operator has 1/16 running and queued tasks
[2024-12-13T18:27:03.299+0100] {scheduler_job_runner.py:495} INFO - DAG example_bash_operator has 2/16 running and queued tasks
[2024-12-13T18:27:03.299+0100] {scheduler_job_runner.py:495} INFO - DAG example_bash_operator has 3/16 running and queued tasks
[2024-12-13T18:27:03.299+0100] {scheduler_job_runner.py:495} INFO - DAG example_bash_operator has 4/16 running and queued tasks
[2024-12-13T18:27:03.299+0100] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: example_bash_operator.runme_0 scheduled__2024-12-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: example_bash_operator.runme_1 scheduled__2024-12-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: example_bash_operator.runme_2 scheduled__2024-12-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: example_bash_operator.also_run_this scheduled__2024-12-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: example_bash_operator.this_will_skip scheduled__2024-12-12T00:00:00+00:00 [scheduled]>
[2024-12-13T18:27:03.301+0100] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: example_bash_operator.runme_0 scheduled__2024-12-12T00:00:00+00:00 [scheduled]>, <TaskInstance: example_bash_operator.runme_1 scheduled__2024-12-12T00:00:00+00:00 [scheduled]>, <TaskInstance: example_bash_operator.runme_2 scheduled__2024-12-12T00:00:00+00:00 [scheduled]>, <TaskInstance: example_bash_operator.also_run_this scheduled__2024-12-12T00:00:00+00:00 [scheduled]>, <TaskInstance: example_bash_operator.this_will_skip scheduled__2024-12-12T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-12-13T18:27:03.302+0100] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='example_bash_operator', task_id='runme_0', run_id='scheduled__2024-12-12T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2024-12-13T18:27:03.302+0100] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'example_bash_operator', 'runme_0', 'scheduled__2024-12-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_bash_operator.py']
[2024-12-13T18:27:03.302+0100] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='example_bash_operator', task_id='runme_1', run_id='scheduled__2024-12-12T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2024-12-13T18:27:03.302+0100] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'example_bash_operator', 'runme_1', 'scheduled__2024-12-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_bash_operator.py']
[2024-12-13T18:27:03.302+0100] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='example_bash_operator', task_id='runme_2', run_id='scheduled__2024-12-12T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2024-12-13T18:27:03.303+0100] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'example_bash_operator', 'runme_2', 'scheduled__2024-12-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_bash_operator.py']
[2024-12-13T18:27:03.303+0100] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='example_bash_operator', task_id='also_run_this', run_id='scheduled__2024-12-12T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-12-13T18:27:03.303+0100] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'example_bash_operator', 'also_run_this', 'scheduled__2024-12-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_bash_operator.py']
[2024-12-13T18:27:03.303+0100] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='example_bash_operator', task_id='this_will_skip', run_id='scheduled__2024-12-12T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-12-13T18:27:03.303+0100] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'example_bash_operator', 'this_will_skip', 'scheduled__2024-12-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_bash_operator.py']
[2024-12-13T18:27:03.318+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'example_bash_operator', 'runme_0', 'scheduled__2024-12-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_bash_operator.py']
/home/adminabhi/gitrepo/airflow_local_server/airflow-venv/lib/python3.10/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
[2024-12-13T18:27:04.321+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/example_bash_operator.py
[2024-12-13T18:27:04.432+0100] {task_command.py:467} INFO - Running <TaskInstance: example_bash_operator.runme_0 scheduled__2024-12-12T00:00:00+00:00 [queued]> on host Abhijit.
[2024-12-13T18:27:06.153+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'example_bash_operator', 'runme_1', 'scheduled__2024-12-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_bash_operator.py']
/home/adminabhi/gitrepo/airflow_local_server/airflow-venv/lib/python3.10/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
[2024-12-13T18:27:07.193+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/example_bash_operator.py
[2024-12-13T18:27:07.345+0100] {task_command.py:467} INFO - Running <TaskInstance: example_bash_operator.runme_1 scheduled__2024-12-12T00:00:00+00:00 [queued]> on host Abhijit.
[2024-12-13T18:27:09.023+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'example_bash_operator', 'runme_2', 'scheduled__2024-12-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_bash_operator.py']
/home/adminabhi/gitrepo/airflow_local_server/airflow-venv/lib/python3.10/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
[2024-12-13T18:27:09.951+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/example_bash_operator.py
[2024-12-13T18:27:10.079+0100] {task_command.py:467} INFO - Running <TaskInstance: example_bash_operator.runme_2 scheduled__2024-12-12T00:00:00+00:00 [queued]> on host Abhijit.
[2024-12-13T18:27:11.787+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'example_bash_operator', 'also_run_this', 'scheduled__2024-12-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_bash_operator.py']
/home/adminabhi/gitrepo/airflow_local_server/airflow-venv/lib/python3.10/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
[2024-12-13T18:27:12.883+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/example_bash_operator.py
[2024-12-13T18:27:13.011+0100] {task_command.py:467} INFO - Running <TaskInstance: example_bash_operator.also_run_this scheduled__2024-12-12T00:00:00+00:00 [queued]> on host Abhijit.
[2024-12-13T18:27:13.663+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'example_bash_operator', 'this_will_skip', 'scheduled__2024-12-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_bash_operator.py']
/home/adminabhi/gitrepo/airflow_local_server/airflow-venv/lib/python3.10/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
[2024-12-13T18:27:14.632+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/example_bash_operator.py
[2024-12-13T18:27:14.739+0100] {task_command.py:467} INFO - Running <TaskInstance: example_bash_operator.this_will_skip scheduled__2024-12-12T00:00:00+00:00 [queued]> on host Abhijit.
[2024-12-13T18:27:15.570+0100] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='example_bash_operator', task_id='runme_0', run_id='scheduled__2024-12-12T00:00:00+00:00', try_number=1, map_index=-1)
[2024-12-13T18:27:15.570+0100] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='example_bash_operator', task_id='runme_1', run_id='scheduled__2024-12-12T00:00:00+00:00', try_number=1, map_index=-1)
[2024-12-13T18:27:15.571+0100] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='example_bash_operator', task_id='runme_2', run_id='scheduled__2024-12-12T00:00:00+00:00', try_number=1, map_index=-1)
[2024-12-13T18:27:15.571+0100] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='example_bash_operator', task_id='also_run_this', run_id='scheduled__2024-12-12T00:00:00+00:00', try_number=1, map_index=-1)
[2024-12-13T18:27:15.571+0100] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='example_bash_operator', task_id='this_will_skip', run_id='scheduled__2024-12-12T00:00:00+00:00', try_number=1, map_index=-1)
[2024-12-13T18:27:15.578+0100] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=example_bash_operator, task_id=also_run_this, run_id=scheduled__2024-12-12T00:00:00+00:00, map_index=-1, run_start_date=2024-12-13 17:27:13.066881+00:00, run_end_date=2024-12-13 17:27:13.274658+00:00, run_duration=0.207777, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=5, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-12-13 17:27:03.300271+00:00, queued_by_job_id=1, pid=4030
[2024-12-13T18:27:15.579+0100] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=example_bash_operator, task_id=runme_0, run_id=scheduled__2024-12-12T00:00:00+00:00, map_index=-1, run_start_date=2024-12-13 17:27:04.486501+00:00, run_end_date=2024-12-13 17:27:05.681905+00:00, run_duration=1.195404, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=2, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-12-13 17:27:03.300271+00:00, queued_by_job_id=1, pid=3982
[2024-12-13T18:27:15.579+0100] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=example_bash_operator, task_id=runme_1, run_id=scheduled__2024-12-12T00:00:00+00:00, map_index=-1, run_start_date=2024-12-13 17:27:07.396615+00:00, run_end_date=2024-12-13 17:27:08.617806+00:00, run_duration=1.221191, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=3, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-12-13 17:27:03.300271+00:00, queued_by_job_id=1, pid=4004
[2024-12-13T18:27:15.580+0100] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=example_bash_operator, task_id=runme_2, run_id=scheduled__2024-12-12T00:00:00+00:00, map_index=-1, run_start_date=2024-12-13 17:27:10.133928+00:00, run_end_date=2024-12-13 17:27:11.350458+00:00, run_duration=1.21653, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=4, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-12-13 17:27:03.300271+00:00, queued_by_job_id=1, pid=4015
[2024-12-13T18:27:15.580+0100] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=example_bash_operator, task_id=this_will_skip, run_id=scheduled__2024-12-12T00:00:00+00:00, map_index=-1, run_start_date=2024-12-13 17:27:14.793607+00:00, run_end_date=2024-12-13 17:27:15.030760+00:00, run_duration=0.237153, state=skipped, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=6, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-12-13 17:27:03.300271+00:00, queued_by_job_id=1, pid=4041
[2024-12-13T18:27:15.654+0100] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: example_bash_operator.run_after_loop scheduled__2024-12-12T00:00:00+00:00 [scheduled]>
[2024-12-13T18:27:15.655+0100] {scheduler_job_runner.py:495} INFO - DAG example_bash_operator has 0/16 running and queued tasks
[2024-12-13T18:27:15.655+0100] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: example_bash_operator.run_after_loop scheduled__2024-12-12T00:00:00+00:00 [scheduled]>
[2024-12-13T18:27:15.656+0100] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: example_bash_operator.run_after_loop scheduled__2024-12-12T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-12-13T18:27:15.657+0100] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='example_bash_operator', task_id='run_after_loop', run_id='scheduled__2024-12-12T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2024-12-13T18:27:15.657+0100] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'example_bash_operator', 'run_after_loop', 'scheduled__2024-12-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_bash_operator.py']
[2024-12-13T18:27:15.671+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'example_bash_operator', 'run_after_loop', 'scheduled__2024-12-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_bash_operator.py']
/home/adminabhi/gitrepo/airflow_local_server/airflow-venv/lib/python3.10/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
[2024-12-13T18:27:16.671+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/example_bash_operator.py
[2024-12-13T18:27:16.791+0100] {task_command.py:467} INFO - Running <TaskInstance: example_bash_operator.run_after_loop scheduled__2024-12-12T00:00:00+00:00 [queued]> on host Abhijit.
[2024-12-13T18:27:17.460+0100] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='example_bash_operator', task_id='run_after_loop', run_id='scheduled__2024-12-12T00:00:00+00:00', try_number=1, map_index=-1)
[2024-12-13T18:27:17.464+0100] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=example_bash_operator, task_id=run_after_loop, run_id=scheduled__2024-12-12T00:00:00+00:00, map_index=-1, run_start_date=2024-12-13 17:27:16.843722+00:00, run_end_date=2024-12-13 17:27:17.045784+00:00, run_duration=0.202062, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=7, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-12-13 17:27:15.655839+00:00, queued_by_job_id=1, pid=4053
[2024-12-13T18:27:17.498+0100] {dagrun.py:854} INFO - Marking run <DagRun example_bash_operator @ 2024-12-12 00:00:00+00:00: scheduled__2024-12-12T00:00:00+00:00, state:running, queued_at: 2024-12-13 17:27:03.170425+00:00. externally triggered: False> successful
[2024-12-13T18:27:17.499+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=example_bash_operator, execution_date=2024-12-12 00:00:00+00:00, run_id=scheduled__2024-12-12T00:00:00+00:00, run_start_date=2024-12-13 17:27:03.243244+00:00, run_end_date=2024-12-13 17:27:17.499699+00:00, run_duration=14.256455, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-12-12 00:00:00+00:00, data_interval_end=2024-12-13 00:00:00+00:00, dag_hash=6b57cae344b1385f6479a8b34a6c9b4e
[2024-12-13T18:27:17.504+0100] {dag.py:4180} INFO - Setting next_dagrun for example_bash_operator to 2024-12-13 00:00:00+00:00, run_after=2024-12-14 00:00:00+00:00
[2024-12-13T18:31:21.218+0100] {scheduler_job_runner.py:1852} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-13T18:36:15.231+0100] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: example_python_operator.print_the_context manual__2024-12-13T17:36:14.512677+00:00 [scheduled]>
[2024-12-13T18:36:15.232+0100] {scheduler_job_runner.py:495} INFO - DAG example_python_operator has 0/16 running and queued tasks
[2024-12-13T18:36:15.233+0100] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: example_python_operator.print_the_context manual__2024-12-13T17:36:14.512677+00:00 [scheduled]>
[2024-12-13T18:36:15.235+0100] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: example_python_operator.print_the_context manual__2024-12-13T17:36:14.512677+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-12-13T18:36:15.239+0100] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='example_python_operator', task_id='print_the_context', run_id='manual__2024-12-13T17:36:14.512677+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 7 and queue default
[2024-12-13T18:36:15.241+0100] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'example_python_operator', 'print_the_context', 'manual__2024-12-13T17:36:14.512677+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_python_operator.py']
[2024-12-13T18:36:15.257+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'example_python_operator', 'print_the_context', 'manual__2024-12-13T17:36:14.512677+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_python_operator.py']
/home/adminabhi/gitrepo/airflow_local_server/airflow-venv/lib/python3.10/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
[2024-12-13T18:36:16.267+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/example_python_operator.py
[2024-12-13T18:36:16.300+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2024-12-13T18:36:16.408+0100] {task_command.py:467} INFO - Running <TaskInstance: example_python_operator.print_the_context manual__2024-12-13T17:36:14.512677+00:00 [queued]> on host Abhijit.
[2024-12-13T18:36:17.127+0100] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='example_python_operator', task_id='print_the_context', run_id='manual__2024-12-13T17:36:14.512677+00:00', try_number=1, map_index=-1)
[2024-12-13T18:36:17.131+0100] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=example_python_operator, task_id=print_the_context, run_id=manual__2024-12-13T17:36:14.512677+00:00, map_index=-1, run_start_date=2024-12-13 17:36:16.455485+00:00, run_end_date=2024-12-13 17:36:16.650232+00:00, run_duration=0.194747, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=8, pool=default_pool, queue=default, priority_weight=7, operator=PythonOperator, queued_dttm=2024-12-13 17:36:15.233831+00:00, queued_by_job_id=1, pid=7527
[2024-12-13T18:36:17.198+0100] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: example_python_operator.log_sql_query manual__2024-12-13T17:36:14.512677+00:00 [scheduled]>
[2024-12-13T18:36:17.199+0100] {scheduler_job_runner.py:495} INFO - DAG example_python_operator has 0/16 running and queued tasks
[2024-12-13T18:36:17.199+0100] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: example_python_operator.log_sql_query manual__2024-12-13T17:36:14.512677+00:00 [scheduled]>
[2024-12-13T18:36:17.200+0100] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: example_python_operator.log_sql_query manual__2024-12-13T17:36:14.512677+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-12-13T18:36:17.201+0100] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='example_python_operator', task_id='log_sql_query', run_id='manual__2024-12-13T17:36:14.512677+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 6 and queue default
[2024-12-13T18:36:17.201+0100] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'example_python_operator', 'log_sql_query', 'manual__2024-12-13T17:36:14.512677+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_python_operator.py']
[2024-12-13T18:36:17.214+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'example_python_operator', 'log_sql_query', 'manual__2024-12-13T17:36:14.512677+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_python_operator.py']
/home/adminabhi/gitrepo/airflow_local_server/airflow-venv/lib/python3.10/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
[2024-12-13T18:36:18.256+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/example_python_operator.py
[2024-12-13T18:36:18.295+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2024-12-13T18:36:18.437+0100] {task_command.py:467} INFO - Running <TaskInstance: example_python_operator.log_sql_query manual__2024-12-13T17:36:14.512677+00:00 [queued]> on host Abhijit.
[2024-12-13T18:36:19.069+0100] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='example_python_operator', task_id='log_sql_query', run_id='manual__2024-12-13T17:36:14.512677+00:00', try_number=1, map_index=-1)
[2024-12-13T18:36:19.072+0100] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=example_python_operator, task_id=log_sql_query, run_id=manual__2024-12-13T17:36:14.512677+00:00, map_index=-1, run_start_date=2024-12-13 17:36:18.483764+00:00, run_end_date=2024-12-13 17:36:18.646990+00:00, run_duration=0.163226, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=9, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-12-13 17:36:17.199770+00:00, queued_by_job_id=1, pid=7537
[2024-12-13T18:36:19.125+0100] {scheduler_job_runner.py:423} INFO - 5 tasks up for execution:
	<TaskInstance: example_python_operator.sleep_for_0 manual__2024-12-13T17:36:14.512677+00:00 [scheduled]>
	<TaskInstance: example_python_operator.sleep_for_1 manual__2024-12-13T17:36:14.512677+00:00 [scheduled]>
	<TaskInstance: example_python_operator.sleep_for_2 manual__2024-12-13T17:36:14.512677+00:00 [scheduled]>
	<TaskInstance: example_python_operator.sleep_for_3 manual__2024-12-13T17:36:14.512677+00:00 [scheduled]>
	<TaskInstance: example_python_operator.sleep_for_4 manual__2024-12-13T17:36:14.512677+00:00 [scheduled]>
[2024-12-13T18:36:19.126+0100] {scheduler_job_runner.py:495} INFO - DAG example_python_operator has 0/16 running and queued tasks
[2024-12-13T18:36:19.126+0100] {scheduler_job_runner.py:495} INFO - DAG example_python_operator has 1/16 running and queued tasks
[2024-12-13T18:36:19.126+0100] {scheduler_job_runner.py:495} INFO - DAG example_python_operator has 2/16 running and queued tasks
[2024-12-13T18:36:19.126+0100] {scheduler_job_runner.py:495} INFO - DAG example_python_operator has 3/16 running and queued tasks
[2024-12-13T18:36:19.126+0100] {scheduler_job_runner.py:495} INFO - DAG example_python_operator has 4/16 running and queued tasks
[2024-12-13T18:36:19.127+0100] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: example_python_operator.sleep_for_0 manual__2024-12-13T17:36:14.512677+00:00 [scheduled]>
	<TaskInstance: example_python_operator.sleep_for_1 manual__2024-12-13T17:36:14.512677+00:00 [scheduled]>
	<TaskInstance: example_python_operator.sleep_for_2 manual__2024-12-13T17:36:14.512677+00:00 [scheduled]>
	<TaskInstance: example_python_operator.sleep_for_3 manual__2024-12-13T17:36:14.512677+00:00 [scheduled]>
	<TaskInstance: example_python_operator.sleep_for_4 manual__2024-12-13T17:36:14.512677+00:00 [scheduled]>
[2024-12-13T18:36:19.128+0100] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: example_python_operator.sleep_for_0 manual__2024-12-13T17:36:14.512677+00:00 [scheduled]>, <TaskInstance: example_python_operator.sleep_for_1 manual__2024-12-13T17:36:14.512677+00:00 [scheduled]>, <TaskInstance: example_python_operator.sleep_for_2 manual__2024-12-13T17:36:14.512677+00:00 [scheduled]>, <TaskInstance: example_python_operator.sleep_for_3 manual__2024-12-13T17:36:14.512677+00:00 [scheduled]>, <TaskInstance: example_python_operator.sleep_for_4 manual__2024-12-13T17:36:14.512677+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-12-13T18:36:19.129+0100] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='example_python_operator', task_id='sleep_for_0', run_id='manual__2024-12-13T17:36:14.512677+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2024-12-13T18:36:19.129+0100] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'example_python_operator', 'sleep_for_0', 'manual__2024-12-13T17:36:14.512677+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_python_operator.py']
[2024-12-13T18:36:19.129+0100] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='example_python_operator', task_id='sleep_for_1', run_id='manual__2024-12-13T17:36:14.512677+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2024-12-13T18:36:19.130+0100] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'example_python_operator', 'sleep_for_1', 'manual__2024-12-13T17:36:14.512677+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_python_operator.py']
[2024-12-13T18:36:19.130+0100] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='example_python_operator', task_id='sleep_for_2', run_id='manual__2024-12-13T17:36:14.512677+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2024-12-13T18:36:19.130+0100] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'example_python_operator', 'sleep_for_2', 'manual__2024-12-13T17:36:14.512677+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_python_operator.py']
[2024-12-13T18:36:19.131+0100] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='example_python_operator', task_id='sleep_for_3', run_id='manual__2024-12-13T17:36:14.512677+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2024-12-13T18:36:19.131+0100] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'example_python_operator', 'sleep_for_3', 'manual__2024-12-13T17:36:14.512677+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_python_operator.py']
[2024-12-13T18:36:19.131+0100] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='example_python_operator', task_id='sleep_for_4', run_id='manual__2024-12-13T17:36:14.512677+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2024-12-13T18:36:19.131+0100] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'example_python_operator', 'sleep_for_4', 'manual__2024-12-13T17:36:14.512677+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_python_operator.py']
[2024-12-13T18:36:19.147+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'example_python_operator', 'sleep_for_0', 'manual__2024-12-13T17:36:14.512677+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_python_operator.py']
/home/adminabhi/gitrepo/airflow_local_server/airflow-venv/lib/python3.10/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
[2024-12-13T18:36:20.083+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/example_python_operator.py
[2024-12-13T18:36:20.112+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2024-12-13T18:36:20.211+0100] {task_command.py:467} INFO - Running <TaskInstance: example_python_operator.sleep_for_0 manual__2024-12-13T17:36:14.512677+00:00 [queued]> on host Abhijit.
[2024-12-13T18:36:20.821+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'example_python_operator', 'sleep_for_1', 'manual__2024-12-13T17:36:14.512677+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_python_operator.py']
/home/adminabhi/gitrepo/airflow_local_server/airflow-venv/lib/python3.10/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
[2024-12-13T18:36:22.023+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/example_python_operator.py
[2024-12-13T18:36:22.054+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2024-12-13T18:36:22.165+0100] {task_command.py:467} INFO - Running <TaskInstance: example_python_operator.sleep_for_1 manual__2024-12-13T17:36:14.512677+00:00 [queued]> on host Abhijit.
[2024-12-13T18:36:22.911+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'example_python_operator', 'sleep_for_2', 'manual__2024-12-13T17:36:14.512677+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_python_operator.py']
/home/adminabhi/gitrepo/airflow_local_server/airflow-venv/lib/python3.10/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
[2024-12-13T18:36:23.875+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/example_python_operator.py
[2024-12-13T18:36:23.905+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2024-12-13T18:36:24.000+0100] {task_command.py:467} INFO - Running <TaskInstance: example_python_operator.sleep_for_2 manual__2024-12-13T17:36:14.512677+00:00 [queued]> on host Abhijit.
[2024-12-13T18:36:24.872+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'example_python_operator', 'sleep_for_3', 'manual__2024-12-13T17:36:14.512677+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_python_operator.py']
/home/adminabhi/gitrepo/airflow_local_server/airflow-venv/lib/python3.10/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
[2024-12-13T18:36:25.967+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/example_python_operator.py
[2024-12-13T18:36:26.015+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2024-12-13T18:36:26.130+0100] {task_command.py:467} INFO - Running <TaskInstance: example_python_operator.sleep_for_3 manual__2024-12-13T17:36:14.512677+00:00 [queued]> on host Abhijit.
[2024-12-13T18:36:27.122+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'example_python_operator', 'sleep_for_4', 'manual__2024-12-13T17:36:14.512677+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_python_operator.py']
/home/adminabhi/gitrepo/airflow_local_server/airflow-venv/lib/python3.10/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
[2024-12-13T18:36:28.166+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/example_python_operator.py
[2024-12-13T18:36:28.195+0100] {example_python_operator.py:93} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2024-12-13T18:36:28.285+0100] {task_command.py:467} INFO - Running <TaskInstance: example_python_operator.sleep_for_4 manual__2024-12-13T17:36:14.512677+00:00 [queued]> on host Abhijit.
[2024-12-13T18:36:29.364+0100] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='example_python_operator', task_id='sleep_for_0', run_id='manual__2024-12-13T17:36:14.512677+00:00', try_number=1, map_index=-1)
[2024-12-13T18:36:29.365+0100] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='example_python_operator', task_id='sleep_for_1', run_id='manual__2024-12-13T17:36:14.512677+00:00', try_number=1, map_index=-1)
[2024-12-13T18:36:29.365+0100] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='example_python_operator', task_id='sleep_for_2', run_id='manual__2024-12-13T17:36:14.512677+00:00', try_number=1, map_index=-1)
[2024-12-13T18:36:29.365+0100] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='example_python_operator', task_id='sleep_for_3', run_id='manual__2024-12-13T17:36:14.512677+00:00', try_number=1, map_index=-1)
[2024-12-13T18:36:29.365+0100] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='example_python_operator', task_id='sleep_for_4', run_id='manual__2024-12-13T17:36:14.512677+00:00', try_number=1, map_index=-1)
[2024-12-13T18:36:29.369+0100] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=example_python_operator, task_id=sleep_for_0, run_id=manual__2024-12-13T17:36:14.512677+00:00, map_index=-1, run_start_date=2024-12-13 17:36:20.261891+00:00, run_end_date=2024-12-13 17:36:20.428796+00:00, run_duration=0.166905, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=10, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-12-13 17:36:19.127878+00:00, queued_by_job_id=1, pid=7541
[2024-12-13T18:36:29.370+0100] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=example_python_operator, task_id=sleep_for_1, run_id=manual__2024-12-13T17:36:14.512677+00:00, map_index=-1, run_start_date=2024-12-13 17:36:22.216880+00:00, run_end_date=2024-12-13 17:36:22.475126+00:00, run_duration=0.258246, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=11, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-12-13 17:36:19.127878+00:00, queued_by_job_id=1, pid=7553
[2024-12-13T18:36:29.371+0100] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=example_python_operator, task_id=sleep_for_2, run_id=manual__2024-12-13T17:36:14.512677+00:00, map_index=-1, run_start_date=2024-12-13 17:36:24.049579+00:00, run_end_date=2024-12-13 17:36:24.410199+00:00, run_duration=0.36062, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=12, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-12-13 17:36:19.127878+00:00, queued_by_job_id=1, pid=7563
[2024-12-13T18:36:29.371+0100] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=example_python_operator, task_id=sleep_for_3, run_id=manual__2024-12-13T17:36:14.512677+00:00, map_index=-1, run_start_date=2024-12-13 17:36:26.182673+00:00, run_end_date=2024-12-13 17:36:26.727422+00:00, run_duration=0.544749, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=13, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-12-13 17:36:19.127878+00:00, queued_by_job_id=1, pid=7573
[2024-12-13T18:36:29.372+0100] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=example_python_operator, task_id=sleep_for_4, run_id=manual__2024-12-13T17:36:14.512677+00:00, map_index=-1, run_start_date=2024-12-13 17:36:28.334375+00:00, run_end_date=2024-12-13 17:36:28.937128+00:00, run_duration=0.602753, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=14, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-12-13 17:36:19.127878+00:00, queued_by_job_id=1, pid=7583
[2024-12-13T18:36:29.410+0100] {scheduler_job_runner.py:1852} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-13T18:36:29.583+0100] {dagrun.py:854} INFO - Marking run <DagRun example_python_operator @ 2024-12-13 17:36:14.512677+00:00: manual__2024-12-13T17:36:14.512677+00:00, state:running, queued_at: 2024-12-13 17:36:14.583585+00:00. externally triggered: True> successful
[2024-12-13T18:36:29.583+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=example_python_operator, execution_date=2024-12-13 17:36:14.512677+00:00, run_id=manual__2024-12-13T17:36:14.512677+00:00, run_start_date=2024-12-13 17:36:15.163630+00:00, run_end_date=2024-12-13 17:36:29.583531+00:00, run_duration=14.419901, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-12-13 17:36:14.512677+00:00, data_interval_end=2024-12-13 17:36:14.512677+00:00, dag_hash=2289b8a3762ce671e46616b76142aad2
[2024-12-13T18:37:00.091+0100] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: example_python_decorator.print_the_context manual__2024-12-13T17:36:59.399218+00:00 [scheduled]>
[2024-12-13T18:37:00.092+0100] {scheduler_job_runner.py:495} INFO - DAG example_python_decorator has 0/16 running and queued tasks
[2024-12-13T18:37:00.092+0100] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: example_python_decorator.print_the_context manual__2024-12-13T17:36:59.399218+00:00 [scheduled]>
[2024-12-13T18:37:00.094+0100] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: example_python_decorator.print_the_context manual__2024-12-13T17:36:59.399218+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-12-13T18:37:00.095+0100] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='example_python_decorator', task_id='print_the_context', run_id='manual__2024-12-13T17:36:59.399218+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 7 and queue default
[2024-12-13T18:37:00.095+0100] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'example_python_decorator', 'print_the_context', 'manual__2024-12-13T17:36:59.399218+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_python_decorator.py']
[2024-12-13T18:37:00.116+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'example_python_decorator', 'print_the_context', 'manual__2024-12-13T17:36:59.399218+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_python_decorator.py']
/home/adminabhi/gitrepo/airflow_local_server/airflow-venv/lib/python3.10/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
[2024-12-13T18:37:01.342+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/example_python_decorator.py
[2024-12-13T18:37:01.382+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2024-12-13T18:37:01.509+0100] {task_command.py:467} INFO - Running <TaskInstance: example_python_decorator.print_the_context manual__2024-12-13T17:36:59.399218+00:00 [queued]> on host Abhijit.
[2024-12-13T18:37:02.176+0100] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='example_python_decorator', task_id='print_the_context', run_id='manual__2024-12-13T17:36:59.399218+00:00', try_number=1, map_index=-1)
[2024-12-13T18:37:02.180+0100] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=example_python_decorator, task_id=print_the_context, run_id=manual__2024-12-13T17:36:59.399218+00:00, map_index=-1, run_start_date=2024-12-13 17:37:01.561336+00:00, run_end_date=2024-12-13 17:37:01.737894+00:00, run_duration=0.176558, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=15, pool=default_pool, queue=default, priority_weight=7, operator=_PythonDecoratedOperator, queued_dttm=2024-12-13 17:37:00.093068+00:00, queued_by_job_id=1, pid=7798
[2024-12-13T18:37:02.398+0100] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
	<TaskInstance: example_python_decorator.log_sql_query manual__2024-12-13T17:36:59.399218+00:00 [scheduled]>
[2024-12-13T18:37:02.398+0100] {scheduler_job_runner.py:495} INFO - DAG example_python_decorator has 0/16 running and queued tasks
[2024-12-13T18:37:02.399+0100] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: example_python_decorator.log_sql_query manual__2024-12-13T17:36:59.399218+00:00 [scheduled]>
[2024-12-13T18:37:02.401+0100] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: example_python_decorator.log_sql_query manual__2024-12-13T17:36:59.399218+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-12-13T18:37:02.401+0100] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='example_python_decorator', task_id='log_sql_query', run_id='manual__2024-12-13T17:36:59.399218+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 6 and queue default
[2024-12-13T18:37:02.402+0100] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'example_python_decorator', 'log_sql_query', 'manual__2024-12-13T17:36:59.399218+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_python_decorator.py']
[2024-12-13T18:37:02.415+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'example_python_decorator', 'log_sql_query', 'manual__2024-12-13T17:36:59.399218+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_python_decorator.py']
/home/adminabhi/gitrepo/airflow_local_server/airflow-venv/lib/python3.10/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
[2024-12-13T18:37:03.636+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/example_python_decorator.py
[2024-12-13T18:37:03.681+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2024-12-13T18:37:03.797+0100] {task_command.py:467} INFO - Running <TaskInstance: example_python_decorator.log_sql_query manual__2024-12-13T17:36:59.399218+00:00 [queued]> on host Abhijit.
[2024-12-13T18:37:04.450+0100] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='example_python_decorator', task_id='log_sql_query', run_id='manual__2024-12-13T17:36:59.399218+00:00', try_number=1, map_index=-1)
[2024-12-13T18:37:04.455+0100] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=example_python_decorator, task_id=log_sql_query, run_id=manual__2024-12-13T17:36:59.399218+00:00, map_index=-1, run_start_date=2024-12-13 17:37:03.846596+00:00, run_end_date=2024-12-13 17:37:04.003614+00:00, run_duration=0.157018, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=16, pool=default_pool, queue=default, priority_weight=6, operator=_PythonDecoratedOperator, queued_dttm=2024-12-13 17:37:02.400075+00:00, queued_by_job_id=1, pid=7810
[2024-12-13T18:37:04.619+0100] {scheduler_job_runner.py:423} INFO - 5 tasks up for execution:
	<TaskInstance: example_python_decorator.sleep_for_0 manual__2024-12-13T17:36:59.399218+00:00 [scheduled]>
	<TaskInstance: example_python_decorator.sleep_for_1 manual__2024-12-13T17:36:59.399218+00:00 [scheduled]>
	<TaskInstance: example_python_decorator.sleep_for_2 manual__2024-12-13T17:36:59.399218+00:00 [scheduled]>
	<TaskInstance: example_python_decorator.sleep_for_3 manual__2024-12-13T17:36:59.399218+00:00 [scheduled]>
	<TaskInstance: example_python_decorator.sleep_for_4 manual__2024-12-13T17:36:59.399218+00:00 [scheduled]>
[2024-12-13T18:37:04.620+0100] {scheduler_job_runner.py:495} INFO - DAG example_python_decorator has 0/16 running and queued tasks
[2024-12-13T18:37:04.620+0100] {scheduler_job_runner.py:495} INFO - DAG example_python_decorator has 1/16 running and queued tasks
[2024-12-13T18:37:04.620+0100] {scheduler_job_runner.py:495} INFO - DAG example_python_decorator has 2/16 running and queued tasks
[2024-12-13T18:37:04.620+0100] {scheduler_job_runner.py:495} INFO - DAG example_python_decorator has 3/16 running and queued tasks
[2024-12-13T18:37:04.620+0100] {scheduler_job_runner.py:495} INFO - DAG example_python_decorator has 4/16 running and queued tasks
[2024-12-13T18:37:04.621+0100] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
	<TaskInstance: example_python_decorator.sleep_for_0 manual__2024-12-13T17:36:59.399218+00:00 [scheduled]>
	<TaskInstance: example_python_decorator.sleep_for_1 manual__2024-12-13T17:36:59.399218+00:00 [scheduled]>
	<TaskInstance: example_python_decorator.sleep_for_2 manual__2024-12-13T17:36:59.399218+00:00 [scheduled]>
	<TaskInstance: example_python_decorator.sleep_for_3 manual__2024-12-13T17:36:59.399218+00:00 [scheduled]>
	<TaskInstance: example_python_decorator.sleep_for_4 manual__2024-12-13T17:36:59.399218+00:00 [scheduled]>
[2024-12-13T18:37:04.622+0100] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: example_python_decorator.sleep_for_0 manual__2024-12-13T17:36:59.399218+00:00 [scheduled]>, <TaskInstance: example_python_decorator.sleep_for_1 manual__2024-12-13T17:36:59.399218+00:00 [scheduled]>, <TaskInstance: example_python_decorator.sleep_for_2 manual__2024-12-13T17:36:59.399218+00:00 [scheduled]>, <TaskInstance: example_python_decorator.sleep_for_3 manual__2024-12-13T17:36:59.399218+00:00 [scheduled]>, <TaskInstance: example_python_decorator.sleep_for_4 manual__2024-12-13T17:36:59.399218+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2024-12-13T18:37:04.622+0100] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='example_python_decorator', task_id='sleep_for_0', run_id='manual__2024-12-13T17:36:59.399218+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2024-12-13T18:37:04.623+0100] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'example_python_decorator', 'sleep_for_0', 'manual__2024-12-13T17:36:59.399218+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_python_decorator.py']
[2024-12-13T18:37:04.623+0100] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='example_python_decorator', task_id='sleep_for_1', run_id='manual__2024-12-13T17:36:59.399218+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2024-12-13T18:37:04.623+0100] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'example_python_decorator', 'sleep_for_1', 'manual__2024-12-13T17:36:59.399218+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_python_decorator.py']
[2024-12-13T18:37:04.623+0100] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='example_python_decorator', task_id='sleep_for_2', run_id='manual__2024-12-13T17:36:59.399218+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2024-12-13T18:37:04.624+0100] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'example_python_decorator', 'sleep_for_2', 'manual__2024-12-13T17:36:59.399218+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_python_decorator.py']
[2024-12-13T18:37:04.624+0100] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='example_python_decorator', task_id='sleep_for_3', run_id='manual__2024-12-13T17:36:59.399218+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2024-12-13T18:37:04.624+0100] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'example_python_decorator', 'sleep_for_3', 'manual__2024-12-13T17:36:59.399218+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_python_decorator.py']
[2024-12-13T18:37:04.625+0100] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='example_python_decorator', task_id='sleep_for_4', run_id='manual__2024-12-13T17:36:59.399218+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2024-12-13T18:37:04.625+0100] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'example_python_decorator', 'sleep_for_4', 'manual__2024-12-13T17:36:59.399218+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_python_decorator.py']
[2024-12-13T18:37:04.640+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'example_python_decorator', 'sleep_for_0', 'manual__2024-12-13T17:36:59.399218+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_python_decorator.py']
/home/adminabhi/gitrepo/airflow_local_server/airflow-venv/lib/python3.10/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
[2024-12-13T18:37:05.616+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/example_python_decorator.py
[2024-12-13T18:37:05.656+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2024-12-13T18:37:05.763+0100] {task_command.py:467} INFO - Running <TaskInstance: example_python_decorator.sleep_for_0 manual__2024-12-13T17:36:59.399218+00:00 [queued]> on host Abhijit.
[2024-12-13T18:37:06.510+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'example_python_decorator', 'sleep_for_1', 'manual__2024-12-13T17:36:59.399218+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_python_decorator.py']
/home/adminabhi/gitrepo/airflow_local_server/airflow-venv/lib/python3.10/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
[2024-12-13T18:37:07.663+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/example_python_decorator.py
[2024-12-13T18:37:07.712+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2024-12-13T18:37:07.826+0100] {task_command.py:467} INFO - Running <TaskInstance: example_python_decorator.sleep_for_1 manual__2024-12-13T17:36:59.399218+00:00 [queued]> on host Abhijit.
[2024-12-13T18:37:08.563+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'example_python_decorator', 'sleep_for_2', 'manual__2024-12-13T17:36:59.399218+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_python_decorator.py']
/home/adminabhi/gitrepo/airflow_local_server/airflow-venv/lib/python3.10/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
[2024-12-13T18:37:09.609+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/example_python_decorator.py
[2024-12-13T18:37:09.658+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2024-12-13T18:37:09.774+0100] {task_command.py:467} INFO - Running <TaskInstance: example_python_decorator.sleep_for_2 manual__2024-12-13T17:36:59.399218+00:00 [queued]> on host Abhijit.
[2024-12-13T18:37:10.617+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'example_python_decorator', 'sleep_for_3', 'manual__2024-12-13T17:36:59.399218+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_python_decorator.py']
/home/adminabhi/gitrepo/airflow_local_server/airflow-venv/lib/python3.10/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
[2024-12-13T18:37:11.604+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/example_python_decorator.py
[2024-12-13T18:37:11.643+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2024-12-13T18:37:11.759+0100] {task_command.py:467} INFO - Running <TaskInstance: example_python_decorator.sleep_for_3 manual__2024-12-13T17:36:59.399218+00:00 [queued]> on host Abhijit.
[2024-12-13T18:37:12.762+0100] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'example_python_decorator', 'sleep_for_4', 'manual__2024-12-13T17:36:59.399218+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_python_decorator.py']
/home/adminabhi/gitrepo/airflow_local_server/airflow-venv/lib/python3.10/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
[2024-12-13T18:37:13.799+0100] {dagbag.py:588} INFO - Filling up the DagBag from /home/adminabhi/gitrepo/usa_customer_shopping_trends/src/dags/example_python_decorator.py
[2024-12-13T18:37:13.837+0100] {example_python_decorator.py:80} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2024-12-13T18:37:13.962+0100] {task_command.py:467} INFO - Running <TaskInstance: example_python_decorator.sleep_for_4 manual__2024-12-13T17:36:59.399218+00:00 [queued]> on host Abhijit.
[2024-12-13T18:37:15.006+0100] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='example_python_decorator', task_id='sleep_for_0', run_id='manual__2024-12-13T17:36:59.399218+00:00', try_number=1, map_index=-1)
[2024-12-13T18:37:15.006+0100] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='example_python_decorator', task_id='sleep_for_1', run_id='manual__2024-12-13T17:36:59.399218+00:00', try_number=1, map_index=-1)
[2024-12-13T18:37:15.007+0100] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='example_python_decorator', task_id='sleep_for_2', run_id='manual__2024-12-13T17:36:59.399218+00:00', try_number=1, map_index=-1)
[2024-12-13T18:37:15.007+0100] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='example_python_decorator', task_id='sleep_for_3', run_id='manual__2024-12-13T17:36:59.399218+00:00', try_number=1, map_index=-1)
[2024-12-13T18:37:15.008+0100] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='example_python_decorator', task_id='sleep_for_4', run_id='manual__2024-12-13T17:36:59.399218+00:00', try_number=1, map_index=-1)
[2024-12-13T18:37:15.012+0100] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=example_python_decorator, task_id=sleep_for_0, run_id=manual__2024-12-13T17:36:59.399218+00:00, map_index=-1, run_start_date=2024-12-13 17:37:05.817760+00:00, run_end_date=2024-12-13 17:37:05.975553+00:00, run_duration=0.157793, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=17, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2024-12-13 17:37:04.621585+00:00, queued_by_job_id=1, pid=7821
[2024-12-13T18:37:15.013+0100] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=example_python_decorator, task_id=sleep_for_1, run_id=manual__2024-12-13T17:36:59.399218+00:00, map_index=-1, run_start_date=2024-12-13 17:37:07.873219+00:00, run_end_date=2024-12-13 17:37:08.127016+00:00, run_duration=0.253797, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=18, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2024-12-13 17:37:04.621585+00:00, queued_by_job_id=1, pid=7831
[2024-12-13T18:37:15.013+0100] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=example_python_decorator, task_id=sleep_for_2, run_id=manual__2024-12-13T17:36:59.399218+00:00, map_index=-1, run_start_date=2024-12-13 17:37:09.821749+00:00, run_end_date=2024-12-13 17:37:10.172522+00:00, run_duration=0.350773, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=19, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2024-12-13 17:37:04.621585+00:00, queued_by_job_id=1, pid=7841
[2024-12-13T18:37:15.014+0100] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=example_python_decorator, task_id=sleep_for_3, run_id=manual__2024-12-13T17:36:59.399218+00:00, map_index=-1, run_start_date=2024-12-13 17:37:11.806933+00:00, run_end_date=2024-12-13 17:37:12.260653+00:00, run_duration=0.45372, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=20, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2024-12-13 17:37:04.621585+00:00, queued_by_job_id=1, pid=7851
[2024-12-13T18:37:15.014+0100] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=example_python_decorator, task_id=sleep_for_4, run_id=manual__2024-12-13T17:36:59.399218+00:00, map_index=-1, run_start_date=2024-12-13 17:37:14.013306+00:00, run_end_date=2024-12-13 17:37:14.566537+00:00, run_duration=0.553231, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=21, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2024-12-13 17:37:04.621585+00:00, queued_by_job_id=1, pid=7861
[2024-12-13T18:37:15.175+0100] {dagrun.py:854} INFO - Marking run <DagRun example_python_decorator @ 2024-12-13 17:36:59.399218+00:00: manual__2024-12-13T17:36:59.399218+00:00, state:running, queued_at: 2024-12-13 17:36:59.427142+00:00. externally triggered: True> successful
[2024-12-13T18:37:15.176+0100] {dagrun.py:905} INFO - DagRun Finished: dag_id=example_python_decorator, execution_date=2024-12-13 17:36:59.399218+00:00, run_id=manual__2024-12-13T17:36:59.399218+00:00, run_start_date=2024-12-13 17:37:00.032347+00:00, run_end_date=2024-12-13 17:37:15.176419+00:00, run_duration=15.144072, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-12-13 17:36:59.399218+00:00, data_interval_end=2024-12-13 17:36:59.399218+00:00, dag_hash=e282f8f786e567a977ffdcff817e61f0
[2024-12-13T18:41:29.450+0100] {scheduler_job_runner.py:1852} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-13T18:46:29.489+0100] {scheduler_job_runner.py:1852} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-13T18:51:29.525+0100] {scheduler_job_runner.py:1852} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-13T18:56:29.558+0100] {scheduler_job_runner.py:1852} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-13T19:01:29.592+0100] {scheduler_job_runner.py:1852} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-12-13T19:06:29.735+0100] {scheduler_job_runner.py:1852} INFO - Adopting or resetting orphaned tasks for active dag runs
